\documentclass[12pt,a4paper]{report}
\usepackage[portuguese]{babel}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[colorlinks,allcolors=blue]{hyperref}
\usepackage{microtype}
\usepackage{graphicx}

\let\emptyset=\varnothing

\newcommand{\tb}{\textbf}
\newcommand{\tbu}[1]{\tb{\textup{#1}}}
\newcommand{\mb}{\mathbf}
\newcommand{\mc}{\mathcal}

\newcommand{\dpar}[1]{\left(#1\right)}
\newcommand{\dsqr}[1]{\left[#1\right]}
\newcommand{\dcur}[1]{\left\{#1\right\}}
\newcommand{\dabs}[1]{\left|#1\right|}
\newcommand{\ang}[1]{\left\langle#1\right\rangle}

\newcommand{\ds}{\displaystyle}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}


\newtheorem{thm}{Teorema}[chapter]
\newtheorem{lem}[thm]{Lema}
\newtheorem{cor}[thm]{Corolário}
\newtheorem{thm*}[thm]{Teorema*}
\newtheorem{lem*}[thm]{Lema*}
\newtheorem{cor*}[thm]{Corolário*}

\DeclareMathOperator{\sen}{sen}
\DeclareMathOperator{\arcsen}{arc sen}
\DeclareMathOperator{\senh}{senh}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\posto}{posto}
\DeclareMathOperator{\sgn}{sgn}

\title{Notas de álgebra linear}
\author{Max Jáuregui}

%\linespread{1.2}
%\linespread{1.213} %11pt
%\linespread{1.241} %12pt

\begin{document}
\maketitle
  Estas notas foram criadas principalmente para meu uso pessoal e pode eventualmente ser usado como um curso elementar de álgebra linear. Todo o conteúdo foi produzido por mim, seguindo como roteiro o livro: E. L. Lima, \textit{Álgebra linear}, 8 ed. (IMPA, Rio de Janeiro, 2012).

  \begin{flushright}
    Max Jáuregui
  \end{flushright}
\tableofcontents

\chapter{Matrizes}

Uma \tb{matriz} de \tb{ordem} $m\times n$ é um arranjo retangular de $mn$ números reais ou complexos que tem $m$ linhas e $n$ colunas. Se $\mb{a}$ é uma matriz $m\times n$, então
$$\mb{a}=\begin{bmatrix}
  a_{11}&\ldots&a_{1n}\\
  \vdots&\vdots&\vdots\\
  a_{m1}&\ldots&a_{mn}\\
\end{bmatrix}\,,$$
em que, para cada $i\in\{1,\ldots,m\}$ e $j\in\{1,\ldots,n\}$, o número $a_{ij}$ é chamado de \tb{elemento} na posição $(i,j)$ da matriz $\mb{a}$.

Formatos especiais de matrizes:

\begin{enumerate}
  \item \tb{Matriz quadrada:} É uma matriz cujo número de linhas é igual ao seu número de colunas. Se $\mb{a}$ é uma matriz quadrada, os elementos $a_{ii}$ são chamados de elementos da \tb{diagonal} de $\mb{a}$. Exemplos:
  $$\begin{bmatrix}
    4&0\\
    2/3&-1
  \end{bmatrix}\,,\quad \begin{bmatrix}
    5&-2&1\\
    0&0&10\\
    2&1&\pi
  \end{bmatrix}\,.$$
  Os elementos da diagonal dessas matrizes são $(4,-1)$ e $(5,0,\pi)$ respectivamente.
  \item \tb{Matriz diagonal:} É uma matriz quadrada cujos elementos fora da diagonal são nulos. Exemplos:
  $$\begin{bmatrix}
    4&0\\
    0&-1
  \end{bmatrix}\,,\quad \begin{bmatrix}
    5&0&0\\
    0&0&0\\
    0&0&\pi
  \end{bmatrix}\,.$$
  \item \tb{Matriz identidade:} É uma matriz diagonal na qual todos os elementos da diagonal são iguais a $1$. A matriz identidade $n\times n$ é denotada por $\mb{I}_n$. Exemplos:
  $$\mb{I}_2=\begin{bmatrix}
    1&0\\
    0&1
  \end{bmatrix}\,,\quad \mb{I}_3=\begin{bmatrix}
    1&0&0\\
    0&1&0\\
    0&0&1
  \end{bmatrix}\,.$$
  \item \tb{Matriz triangular superior:} É uma matriz quadrada na qual todos os elementos debaixo da diagonal são nulos. Exemplos:
  $$\begin{bmatrix}
    4&3\\
    0&-1
  \end{bmatrix}\,,\quad \begin{bmatrix}
    5&3&0\\
    0&0&2\\
    0&0&\pi
  \end{bmatrix}\,.$$
  \item \tb{Matriz triangular inferior:} É uma matriz quadrada na qual todos os elementos acima da diagonal são nulos. Exemplos:
  $$\begin{bmatrix}
    4&0\\
    3&-1
  \end{bmatrix}\,,\quad \begin{bmatrix}
    5&0&0\\
    3&0&0\\
    0&2&\pi
  \end{bmatrix}\,.$$
  \item \tb{Matriz simétrica:} É uma matriz quadrada $\mb{a}$ tal que $a_{ij}=a_{ji}$ para quaisquer $i$ e $j$, ou seja, é uma matriz simétrica em relação à sua diagonal. Exemplos:
  $$\begin{bmatrix}
    4&3\\
    3&-1
  \end{bmatrix}\,,\quad \begin{bmatrix}
    5&3&0\\
    3&0&2\\
    0&2&\pi
  \end{bmatrix}\,.$$
  \item \tb{Matriz antissimétrica:} É uma matriz quadrada $\mb{a}$ tal que $a_{ij}=-a_{ji}$ para quaisquer $i$ e $j$. Segue daqui que $a_{ii}=-a_{ii}$ para todo $i$, o que implica que os elementos da diagonal de $\mb a$ são nulos. Exemplos:
  $$\begin{bmatrix}
    0&-3\\
    3&0
  \end{bmatrix}\,,\quad \begin{bmatrix}
    0&3&0\\
    -3&0&2\\
    0&-2&0
  \end{bmatrix}\,.$$
  \item \tb{Matriz hermitiana:} É uma matriz quadrada $\mb a$ tal que $a_{ij}=\overline{a_{ji}}$ para quaisquer $i$ e $j$, em que $\overline{a_{ji}}$ denota o complexo conjugado de $a_{ji}$ ($\overline{x+iy}=x-iy$, em que $i^2=-1$). Segue daqui que $a_{ii}=\overline{a_{ii}}$ para todo $i$, o que implica que os elementos da diagonal de $\mb a$ são reais. Exemplos:
  $$\begin{bmatrix}
    4&3i\\
    -3i&-1
  \end{bmatrix}\,,\quad \begin{bmatrix}
    5&3+i&0\\
    3-i&0&-2i\\
    0&2i&\pi
  \end{bmatrix}\,.$$
\end{enumerate}

Como veremos mais na frente, o conjunto de todas as matrizes $m\times n$ é um espaço vetorial, pois nele podem ser definidas duas operações: adição de matrizes e multiplicação de uma matriz por um número (real ou complexo dependendo das circunstâncias). As seguintes equações definem essas operações:
$$\mb{a}+\mb{b}=\begin{bmatrix}
  a_{11}+b_{11}&\ldots&a_{1n}+b_{1n}\\
  \vdots&\vdots&\vdots\\
  a_{m1}+b_{m1}&\ldots&a_{mn}+b_{mn}
\end{bmatrix}\,,\quad
\alpha\mb{a}=\begin{bmatrix}
  \alpha a_{11}&\ldots&\alpha a_{1n}\\
  \vdots&\vdots&\vdots\\
  \alpha a_{m1}&\ldots&\alpha a_{mn}
\end{bmatrix}\,.$$
Pode-se verificar facilmente que essas operações têm as seguintes propriedades:
\begin{enumerate}
  \item Associatividade: $\mb{a}+(\mb{b}+\mb{c})=(\mb{a}+\mb b)+\mb c$ e $\alpha(\beta \mb a)=(\alpha\beta)\mb a$.
  \item Comutatividade: $\mb a+\mb b=\mb b+\mb a$.
  \item Elemento neutro: Existe uma matriz $\mb 0$ tal que $\mb a+\mb 0=\mb a$ para qualquer $\mb a$.
  \item Elemento inverso: Para cada $\mb a$ existe uma matriz $-\mb a$ tal que $\mb a+(-\mb a)=\mb 0$.
  \item Multiplicação por $1$: $1\mb a=\mb a$.
  \item Distributividade: $\alpha(\mb a+\mb b)=\alpha\mb a+\alpha\mb b$ e $(\alpha+\beta)\mb a=\alpha\mb a+\beta\mb a$.
\end{enumerate}

Define-se a \tb{transposta} de uma matriz $\mb a$ como a matriz $\mb a^T$ cujas linhas são as colunas de $\mb a$, ou seja, $a_{ij}^T=a_{ji}$ para quaisquer $i$ e $j$. Segue diretamente dessa definição que $(\mb a^T)^T=\mb a$, $(\mb a+\mb b)^T=\mb a^T+\mb b^T$ e $(\alpha\mb a)^T=\alpha\mb a^T$.

Exemplos:

\begin{enumerate}
  \item Uma matriz $\mb a$ é simétrica (antissimétrica) se, e somente se, $\mb a=\mb a^T$ ($\mb a=-\mb a^T$).
  \item Uma matriz $\mb a$ é hermitiana se, e somente se, $\mb a^T=\overline{\mb a}$, em que $\overline{\mb a}$ é a matriz obtida ao tomar o complexo conjugado de cada elemento de $\mb a$.
  \item Toda matriz quadrada $\mb a$ pode ser escrita como a soma de uma matriz simétrica e uma matriz antissimétrica. Com efeito, definindo $\mb s=(\mb a+\mb a^T)/2$ e $\mb t=(\mb a-\mb a^T)/2$, temos que $\mb s=\mb s^T$, $\mb t=-\mb t^T$ e $\mb a=\mb s+\mb t$.
\end{enumerate}

Além das duas operações anteriores, pode ser definida uma \tb{multiplicação de matrizes}. Se $\mb a$ é uma matriz $m\times n$ e $\mb b$ é uma matriz $n\times p$, então a multiplicação de $\mb a$ com $\mb b$ pode ser efetuada e o produto é uma matriz $\mb c$ de ordem $m\times p$ tal que
$$c_{ik}=\sum_{j=1}^na_{ij}b_{jk}=a_{i1}b_{1k}+\cdots+a_{in}b_{nk}\,.$$
Em outras palavras, o elemento $c_{ik}$ da matriz produto $\mb c$ é obtido multiplicando cada elemento da linha $i$ da matriz $\mb a$ com o elemento respectivo na coluna $k$ da matriz $\mb b$ e somando esses produtos.

Exemplos:
\begin{enumerate}
  \item Dadas as matrizes
  $$\mb a=\begin{bmatrix}
    1&0&-3\\
    2&1&0
  \end{bmatrix}\quad\text{e}\quad\mb b=\begin{bmatrix}
    1&5&0\\
    4&-1&3\\
    0&2&0
  \end{bmatrix}\,,$$
  temos que
  $$\mb {ab}=\begin{bmatrix}
    1&-1&0\\
    6&9&3
  \end{bmatrix}\,.$$
  No entanto, o produto $\mb{ba}$ não está definido, pois o número de linhas de $\mb a$ é diferente do número de colunas de $\mb b$.
  \item Se $\mb a$ é uma matriz $m\times n$, o produto de $\mb a$ com $\mb I_n$ é o próprio $\mb a$ (isso justifica o nome de matriz identidade). Com efeito, os elementos da matriz $\mb I_n$ são dados pelo \tb{símbolo de Kronecker}
  $$\delta_{ij}=\begin{cases}
    1&\text{se $i=j$}\\
    0&\text{se $i\ne j$.}
  \end{cases}$$
  Logo, se $\mb c=\mb {aI}_n$, então $c_{ik}=\sum_{j=1}^na_{ij}\delta_{jk}=a_{ik}\delta_{kk}=a_{ik}$. De forma análoga pode-se provar que $\mb I_m\mb a=\mb a$.
  \item Duas matrizes não nulas podem ter como produto a matriz nula. Com efeito,
  $$\begin{bmatrix}
    1&0&1\\
    0&-1&2
  \end{bmatrix}\begin{bmatrix}
    -1&3\\
    2&-6\\
    1&-3
  \end{bmatrix}=\begin{bmatrix}
    0&0\\
    0&0
  \end{bmatrix}\,.$$
  Por outro lado
  $$\begin{bmatrix}
    -1&3\\
    2&-6\\
    1&-3
  \end{bmatrix}\begin{bmatrix}
    1&0&1\\
    0&-1&2
  \end{bmatrix}=\begin{bmatrix}
    -1&-3&5\\
    2&6&-10\\
    1&3&-5
  \end{bmatrix}\,,$$
  o que mostra que a multiplicação de matrizes não é comutativa.
  \item Mesmo a multiplicação de duas matrizes quadradas não é comutativa, pois, por exemplo,
  $$\begin{bmatrix}
    2&3\\
    -1&0
  \end{bmatrix} \begin{bmatrix}
    0&1\\
    1&0
  \end{bmatrix}=\begin{bmatrix}
    3&2\\
    0&-1
  \end{bmatrix}\quad\text{e}\quad\begin{bmatrix}
    0&1\\
    1&0
  \end{bmatrix} \begin{bmatrix}
    2&3\\
    -1&0
  \end{bmatrix}= \begin{bmatrix}
    -1&0\\
    2&3
  \end{bmatrix}\,.$$
\end{enumerate}

\begin{thm}
  Sejam $\mb a$ e $\mb b$ matrizes $m\times n$, $\mb c$ e $\mb d$ matrizes $n\times p$ e $\alpha$ um número. Tem-se que $\mb a(\mb b+\mb c)=\mb {ab}+\mb{ac}$, $(\mb a+\mb b)\mb c=\mb {ac}+\mb{bc}$ e $\mb a(\alpha\mb b)=\alpha (\mb {ab})=(\alpha \mb a)\mb b$.
\end{thm}
\begin{proof}
  Se $\mb p=\mb a(\mb b+\mb c)$, então
  $$p_{ik}=\sum_{j=1}^na_{ij}(b_{jk}+c_{jk})=\sum_{j=1}^na_{ij}b_{jk}+\sum_{j=1}^na_{ij}c_{jk}\,,$$
  o que implica que $\mb p=\mb {ab}+\mb{ac}$. A igualdade $(\mb a+\mb b)\mb c=\mb {ac}+\mb{bc}$ pode ser provada de forma análoga.
\end{proof}

Exemplo: Dada a matriz
$$\mb a=\begin{bmatrix}
  1&x\\
  0&1
\end{bmatrix}\,,$$
temos que
$$\mb a^n=\begin{bmatrix}
  1&nx\\
  0&1
\end{bmatrix}$$
para todo $n\in\N$. Com efeito, definindo
$$\mb t=\begin{bmatrix}
  0&x\\
  0&0
\end{bmatrix}$$
temos que $\mb a=\mb I_2+\mb t$ e que $\mb t^2=\mb 0$. Logo,
$$\mb a^2=(\mb I_2+\mb t)^2=(\mb I_2+\mb t)(\mb I_2+\mb t)=\mb I_2^2+\mb I_2\mb t+\mb t\mb I_2+\mb t^2=\mb I_2+2\mb t\,.$$
Supondo que $\mb a^n=\mb I_2+n\mb t$ para algum $n\in\N$, temos que
$$\mb a^{n+1}=\mb a^n\mb a=(\mb I_2+n\mb t)(\mb I_2+\mb t)=\mb I_2^2+\mb I_2\mb t+n\mb t\mb I_2+n\mb t^2=\mb I_2+(n+1)\mb t\,.$$
Portanto, $\mb a^n=\mb I_2+n\mb t$ para todo $n\in\N$.

\begin{thm}
  Sejam $\mb a$ uma matriz $m\times n$, $\mb b$ uma matriz $n\times p$ e $\mb c$ uma matriz $p\times q$. Tem-se que $\mb a(\mb {bc})=(\mb{ab})\mb c$.
\end{thm}
\begin{proof}
  Sejam $\mb r=\mb{ab}$, $\mb s=\mb {bc}$ e $\mb t=\mb {as}$. Temos que
  $$t_{ik}=\sum_{j=1}^na_{ij}s_{jk}=\sum_{j=1}^na_{ij}\sum_{l=1}^pb_{jl}c_{lk}=\sum_{j=1}^n\sum_{l=1}^pa_{ij}b_{jl}c_{lk}=\sum_{l=1}^p\dpar{\sum_{j=1}^na_{ij}b_{jl}}c_{lk}=\sum_{l=1}^pr_{il}c_{lk}\,.$$
  Portanto, $\mb t=\mb {rc}$.
\end{proof}

\begin{thm}
  Sejam $\mb a$ uma matriz $m\times n$ e $\mb b$ uma matriz $n\times p$. Tem-se que $(\mb a\mb b)^T=\mb b^T\mb a^T$.
\end{thm}
\begin{proof}
  Se $\mb c=\mb {ab}$, então
  $$c^T_{ik}=c_{ki}=\sum_{j=1}^na_{kj}b_{ji}=\sum_{j=1}^na_{jk}^Tb_{ij}^T=\sum_{j=1}^nb_{ij}^Ta_{jk}^T\,.$$
  Portanto, $\mb c^T=\mb b^T\mb a^T$.
\end{proof}

Define-se o \tb{traço} de uma matriz quadrada $\mb a$ como a soma dos elementos da sua diagonal. O traço de $\mb a$ é denotado por $\tr(\mb a)$. Segue imediatamente dessa definição que $\tr(\mb a)=\tr(\mb a^T)$, pois $\mb a$ e $\mb a^T$ têm a mesma diagonal. Além disso, $\tr(\mb a+\mb b)=\tr(\mb a)+\tr(\mb b)$ e $\tr(\alpha\mb a)=\alpha\tr(\mb a)$.

\begin{thm}
  Sejam $\mb a$ uma matriz $m\times n$ e $\mb b$ uma matriz $n\times m$. Tem-se que $\tr(\mb {ab})=\tr(\mb{ba})$.
\end{thm}
\begin{proof}
  Se $\mb c=\mb a\mb b$, então $\tr(\mb {ab})=\sum_{i=1}^m c_{ii}$. Como $c_{ii}=\sum_{j=1}^na_{ij}b_{ji}$, segue que
  $$\tr(\mb{ab})=\sum_{i=1}^m\sum_{j=1}^na_{ij}b_{ji}=\sum_{j=1}^n\sum_{i=1}^ma_{ij}b_{ji}=\sum_{j=1}^n\sum_{i=1}^mb_{ji}a_{ij}\,.$$
  Portanto, $\tr(\mb{ab})=\tr(\mb{ba})$.
\end{proof}

\chapter{O método de eliminação de Gauss}

Diz-se que uma matriz é \tb{escalonada} quando o primeiro elemento não-nulo de cada linha está à esquerda dos primeiros elementos não-nulos das linhas subsequentes. Dessa maneira, debaixo do primeiro elemento não-nulo de cada linha só se tem zeros. Exemplos:
$$\begin{bmatrix}
  2&-1&0&4\\
  0&2&3&-1\\
  0&0&0&5
\end{bmatrix}\,,\quad \begin{bmatrix}
  0&1&2&3&4\\
  0&0&0&3&5\\
  0&0&0&0&1\\
  0&0&0&0&0
\end{bmatrix}\,.$$

Vamos agora descrever o \tb{método de eliminação de Gauss} que nos permite transformar uma matriz qualquer em uma matriz escalonada. O método de eliminação consiste em usar sistematicamente as seguintes operações nas linhas da matriz até obter uma matriz escalonada:
\begin{enumerate}
  \item permutar duas linhas;
  \item multiplicar uma linha por um número diferente de $0$;
  \item somar a uma linha um múltiplo de uma outra linha.
\end{enumerate}
Essas operações são às vezes chamadas de \tb{operações elementares}.

Exemplo:
\begin{multline*}
  \begin{bmatrix}
    0&0&2&0\\
    0&1&3&1\\
    0&3&0&2
  \end{bmatrix}\xrightarrow{L_1\leftrightarrow L_2} \begin{bmatrix}
    0&1&3&1\\
    0&0&2&0\\
    0&3&0&2
\end{bmatrix}\xrightarrow{L_3-3L_1} \begin{bmatrix}
  0&1&3&1\\
  0&0&2&0\\
  0&0&-9&-1
\end{bmatrix}\xrightarrow{L_2/2}\begin{bmatrix}
  0&1&3&1\\
  0&0&1&0\\
  0&0&-9&-1
\end{bmatrix}\\
\xrightarrow{L_3+9L_2}\begin{bmatrix}
  0&1&3&1\\
  0&0&1&0\\
  0&0&0&-1
\end{bmatrix}\,.
\end{multline*}

Uma operação elementar na linha de uma matriz $\mb a$ pode ser vista também como o resultado da multiplicação de uma matriz $\mb e$ com $\mb a$. Com efeito, se
$$\mb{e}_1=\begin{bmatrix}
  0&1&0\\
  1&0&0\\
  0&0&1
\end{bmatrix}\quad\text{e}\quad \mb a=\begin{bmatrix}
  0&0&2&0\\
  0&1&3&1\\
  0&3&0&2
\end{bmatrix}\,,$$
temos que
$$\mb{e}_1\mb a=\begin{bmatrix}
  0&1&3&1\\
  0&0&2&0\\
  0&3&0&2
\end{bmatrix}\,,$$
ou seja, multiplicar $\mb e_1$ pela esquerda de $\mb a$ faz permutar as linhas $1$ e $2$ de $\mb a$. Se
$$\mb e_2=\begin{bmatrix}
  1&0&0\\
  0&1&0\\
  -3&0&1
\end{bmatrix}\,,$$
então
$$\mb e_2\mb e_1\mb a=\begin{bmatrix}
  0&1&3&1\\
  0&0&2&0\\
  0&0&-9&-1
\end{bmatrix}\,.$$
Logo, multiplicar $\mb e_2$ pela esquerda de $\mb e_1\mb a$ é equivalente a fazer a operação elementar $L_3-3L_1$ na matriz $\mb e_1\mb a$. Finalmente, se
$$\mb e_3=\begin{bmatrix}
  1&0&0\\
  0&1&0\\
  0&9/2&1
\end{bmatrix}\,,$$
então
$$\mb e_3\mb e_2\mb e_1\mb a=\begin{bmatrix}
  0&1&3&1\\
  0&0&2&0\\
  0&0&0&-1
\end{bmatrix}\,.$$
Dessa maneira a matriz $\mb a$ pode ser transformada numa matriz escalonada se multiplicamos a ela sucessivamente as matrizes $\mb e_1,\mb e_2,\mb e_3$ pela esquerda. As matrizes $\mb e_1,\mb e_2,\mb e_3$ são às vezes chamadas de \tb{matrizes elementares}, pois correspondem a operações elementares nas linhas da matriz $\mb a$.


Define-se o \tb{posto} de uma matriz como o número de linhas não-nulas da matriz escalonada obtida usando o método de eliminação. Segue imediatamente daqui que o posto de uma matriz $m\times n$ é no máximo igual a $m$. O posto de uma matriz não depende dos detalhes do processo de eliminação (isso será justificado mais na frente). Por exemplo, a matriz considerada no exemplo anterior tem posto $3$.

Diz-se que uma matriz $\mb a$ de ordem $n\times n$ é \tb{invertível} se existe uma matriz $\mb a^{-1}$ de ordem $n\times n$, chamada de \tb{inversa} da matriz $\mb a$, tal que $\mb a\mb a^{-1}=\mb a^{-1}\mb a=\mb I_n$. Como será justificado mais na frente, uma matriz quadrada é invertível se, e somente se, seu posto é igual ao seu número de colunas (ou linhas).

Exemplos:
\begin{enumerate}
  \item A matriz
  $$\mb a=\begin{bmatrix}
    1&2&3\\
    4&5&6\\
    7&8&9
  \end{bmatrix}$$
  não é invertível. Com efeito, temos que
  $$\begin{bmatrix}
    1&2&3\\
    4&5&6\\
    7&8&9
  \end{bmatrix}\xrightarrow[L_3-7L_1]{L_2-4L_1} \begin{bmatrix}
    1&2&3\\
    0&-3&-6\\
    0&-6&-12
  \end{bmatrix}\xrightarrow{L3-2L_2} \begin{bmatrix}
    1&2&3\\
    0&-3&-6\\
    0&0&0
  \end{bmatrix}\,,$$
  do qual segue que $\posto(\mb a)=2\ne 3$.
  \item A matriz
  $$\mb a=\begin{bmatrix}
    1&2&3&1\\
    0&4&0&0\\
    3&2&0&2\\
    1&-1&0&1
  \end{bmatrix}$$
  é invertível. Com efeito, temos que
  \begin{multline*}
    \begin{bmatrix}
      1&2&3&1\\
      0&4&0&0\\
      3&2&0&2\\
      1&-1&0&1
    \end{bmatrix}\xrightarrow[L_4-L_1]{L_3-3L_1} \begin{bmatrix}
      1&2&3&1\\
      0&4&0&0\\
      0&-4&-9&-1\\
      0&-3&-3&0
    \end{bmatrix}\xrightarrow[L_4+(3/4)L_2]{L_3+L_2} \begin{bmatrix}
      1&2&3&1\\
      0&4&0&0\\
      0&0&-9&-1\\
      0&0&-3&0
    \end{bmatrix}\\
    \xrightarrow{L_4-L_3/3}\begin{bmatrix}
      1&2&3&1\\
      0&4&0&0\\
      0&0&-9&-1\\
      0&0&0&1/3
    \end{bmatrix}\,,
  \end{multline*}
  o que implica que $\posto(\mb a)=4$.
\end{enumerate}

Um sistema linear de $m$ equações a $n$ incógnitas é dado por
\begin{equation}
  \label{sislin}
  \begin{split}
    a_{11}x_1+\cdots+a_{1n}x_n&=b_1\\
    \vdots\qquad\qquad&\quad\;\;\vdots\\
    a_{m1}x_1+\cdots+a_{mn}x_n&=b_m\,.
  \end{split}
\end{equation}
Diz-se que uma lista $(\alpha_1,\ldots,\alpha_n)$ de $n$ números é uma \tb{solução} do sistema~(\ref{sislin}) se ao substituir $x_i$ por $\alpha_i$, $i\in\{1,\ldots,n\}$, em (\ref{sislin}) obtém-se identidades.

Exemplo: O sistema linear
\begin{equation*}
  \begin{split}
    2x-y=0\\
    x+2y=5
  \end{split}
\end{equation*}
tem como única solução o par ordenado $(1,2)$, pois $2\cdot 1-2=0$ e $1+2\cdot 2=5$.

Um sistema linear pode ter uma única solução, pode ter infinitas soluções ou pode não ter solução. Por exemplo, o sistema linear
\begin{equation*}
  \begin{split}
    2x-y=0\\
    4x-2y=0
  \end{split}
\end{equation*}
tem infinitas soluções, pois o par ordenado $(\alpha,2\alpha)$ é uma solução para qualquer $\alpha\in\R$. Por outro lado, o sistema linear
\begin{equation*}
  \begin{split}
    2x-y=0\\
    4x-2y=1
  \end{split}
\end{equation*}
não tem solução.

Definindo as matrizes
$$\mb a=\begin{bmatrix}
  a_{11}&\ldots&a_{1n}\\
  \vdots&&\vdots\\
  a_{m1}&\ldots&a_{mn}
\end{bmatrix}\,,\quad \mb x=\begin{bmatrix}
  x_1\\
  \vdots\\
  x_n
\end{bmatrix}\quad \text{e}\quad \mb b=\begin{bmatrix}
  b_1\\
  \vdots\\
  b_m
\end{bmatrix}\,,$$
o sistema~(\ref{sislin}) pode ser escrito de forma matricial como $\mb a\mb x=\mb b$. O método de eliminação pode ser usado para determinar se esse sistema tem uma única solução ou se tem infinitas soluções ou se não tem solução. Além disso, nos casos em que há solução, podemos ainda encontrar elas de forma explícita.

Toda a informação contida no sistema~(\ref{sislin}) está contida na chamada \tb{matriz aumentada} do sistema:
$$[\mb a|\mb b]=\begin{bmatrix}
  a_{11}&\ldots&a_{1n}&|&b_1\\
  \vdots&&\vdots&|&\vdots\\
  a_{m1}&\ldots&a_{mn}&|&b_m
\end{bmatrix}\,.$$
As operações elementares que podem ser usadas nessa matriz são equivalentes a
\begin{enumerate}
  \item permutar duas equações do sistema;
  \item multiplicar uma equação do sistema por um número;
  \item somar a uma equação do sistema um múltiplo de outra equação do sistema.
\end{enumerate}
Usando o método de eliminação na matriz aumentada da matriz podemos encontrar o seu posto e simultaneamente o posto da matriz $\mb a$. Com isso temos os seguintes casos:
\begin{enumerate}
  \item se $\posto([\mb a|\mb b])=\posto(\mb a)=n$, o sistema tem uma única solução;
  \item se $\posto([\mb a|\mb b])=\posto(\mb a)<n$, o sistema tem infinitas soluções;
  \item se $\posto([\mb a|\mb b])\ne\posto(\mb a)$, o sistema não tem solução.
\end{enumerate}

Exemplos:
\begin{enumerate}
  \item Seja o sistema linear
  \begin{equation*}
    \begin{split}
      x+y+z&=3\\
      2x-y+2z&=3\\
      x-3y+2z&=0\,.
    \end{split}
  \end{equation*}
  A matriz aumentada desse sistema é
  $$[\mb a|\mb b]=\begin{bmatrix}
    1&1&1&|&3\\
    2&-1&2&|&3\\
    1&-3&2&|&0
  \end{bmatrix}\,.$$
  Logo,
  $$\begin{bmatrix}
    1&1&1&|&3\\
    2&-1&2&|&3\\
    1&-3&2&|&0
  \end{bmatrix}\xrightarrow[L_3-L1]{L_2-2L_1} \begin{bmatrix}
    1&1&1&|&3\\
    0&-3&0&|&-3\\
    0&-4&1&|&-3
  \end{bmatrix}\xrightarrow{L_4-(4/3)L_3} \begin{bmatrix}
    1&1&1&|&3\\
    0&-3&0&|&-3\\
    0&0&1&|&1
  \end{bmatrix}\,.$$
  Segue daqui que $\posto([\mb a|\mb b])=\posto(\mb a)=3$ e, por conseguinte, o sistema tem uma única solução. Para determinar essa solução escrevemos a matriz escalonada obtida como o seguinte sistema linear:
  \begin{equation*}
    \begin{split}
      x+y+z&=3\\
      -3y&=-3\\
      z&=1\,.
    \end{split}
  \end{equation*}
  Resolvendo esse sistema de baixo para cima, obtemos que a solução do sistema é $(1,1,1)$.
  \item Seja o sistema linear
  \begin{equation*}
    \begin{split}
      x+y+z&=3\\
      2x-y+2z&=3\\
      -x+5y-z&=3\,.
    \end{split}
  \end{equation*}
  A matriz aumentada desse sistema é
  $$[\mb a|\mb b]=\begin{bmatrix}
    1&1&1&|&3\\
    2&-1&2&|&3\\
    -1&5&-1&|&3
  \end{bmatrix}\,.$$
  Logo,
  $$\begin{bmatrix}
    1&1&1&|&3\\
    2&-1&2&|&3\\
    -1&5&-1&|&3
  \end{bmatrix}\xrightarrow[L_3+L_1]{L_2-2L_1} \begin{bmatrix}
    1&1&1&|&3\\
    0&-3&0&|&-3\\
    0&6&0&|&6
  \end{bmatrix}\xrightarrow{L_3+2L_2} \begin{bmatrix}
    1&1&1&|&3\\
    0&-3&0&|&-3\\
    0&0&0&|&0
  \end{bmatrix}\,.$$
  Segue daqui que $\posto([\mb a|\mb b])=\posto(\mb a)=2<3$. Assim, o sistema tem infinitas soluções. Para ver a forma dessas soluções escrevemos a matriz escalonada obtida como o seguinte sistema linear:
  \begin{equation*}
    \begin{split}
      x+y+z&=3\\
      -3y&=-3\,.
    \end{split}
  \end{equation*}
  Da segunda equação obtemos que $y=1$. Substituindo isso na primeira, obtemos que $z=2-x$. Logo, a lista $(\alpha,1,2-\alpha)$ é uma solução do sistema para todo $\alpha\in\R$.
  \item Seja o sistema linear
  \begin{equation*}
    \begin{split}
      x+y+z&=3\\
      2x-y+2z&=3\\
      -x+5y-z&=2\,.
    \end{split}
  \end{equation*}
  A matriz aumentada desse sistema é
  $$[\mb a|\mb b]=\begin{bmatrix}
    1&1&1&|&3\\
    2&-1&2&|&3\\
    -1&5&-1&|&2
  \end{bmatrix}\,.$$
  Logo,
  $$\begin{bmatrix}
    1&1&1&|&3\\
    2&-1&2&|&3\\
    -1&5&-1&|&2
  \end{bmatrix}\xrightarrow[L_3+L_1]{L_2-2L_1} \begin{bmatrix}
    1&1&1&|&3\\
    0&-3&0&|&-3\\
    0&6&0&|&5
  \end{bmatrix}\xrightarrow{L_3+2L_2} \begin{bmatrix}
    1&1&1&|&3\\
    0&-3&0&|&-3\\
    0&0&0&|&-1
  \end{bmatrix}\,.$$
  Segue daqui que $\posto([\mb a|\mb b])=3\ne 2=\posto(\mb a)$ e, por conseguinte, o sistema não tem solução. Uma outra forma de chegar nessa conclusão é escrevendo a matriz escalonada obtida como um sistema linear. Nesse sistema a terceira equação diz é $0=-1$, o que é absurdo.
\end{enumerate}

Como vimos anteriormente, o método de eliminação pode ser usado para determinar se uma matriz quadrada $\mb a$ é invertível. Além disso, no caso afirmativo, o método de eliminação pode ser usado para o cálculo efetivo da inversa de $\mb a$.

Sejam as matrizes
$$\mb a=\begin{bmatrix}
  a_{11}&\ldots&a_{1n}\\
  \vdots&&\vdots\\
  a_{n1}&\ldots&a_{nn}
\end{bmatrix}\quad\text{e}\quad \mb x=\begin{bmatrix}
  x_{11}&\ldots&x_{1n}\\
  \vdots&&\vdots\\
  x_{n1}&\ldots&x_{nn}
\end{bmatrix}\,.$$
Para que $\mb x$ seja a matriz inversa de $\mb a$, deve-se ter em particular que $\mb a\mb x=\mb I_n$. Logo, se queremos encontrar os elementos da matriz $\mb x$, temos que resolver $n$ sistemas lineares com $n$ equações e $n$ incógnitas. O primeiro sistema corresponderia à obtenção da primeira coluna de $\mb I_n$, ou seja,
\begin{equation*}
  \begin{split}
    a_{11}x_{11}+\cdots+a_{1n}x_{n1}&=1\\
    a_{21}x_{11}+\cdots+a_{2n}x_{n1}&=0\\
    \vdots\qquad\qquad&\quad\,\,\,\vdots\\
    a_{n1}x_{11}+\cdots+a_{nn}x_{n1}&=0\,.
  \end{split}
\end{equation*}
O segundo sistema, que corresponde à obtenção da segunda coluna de $\mb I_n$, é
\begin{equation*}
  \begin{split}
    a_{11}x_{12}+\cdots+a_{1n}x_{n2}&=0\\
    a_{21}x_{12}+\cdots+a_{2n}x_{n2}&=1\\
    \vdots\qquad\qquad&\quad\,\,\,\vdots\\
    a_{n1}x_{12}+\cdots+a_{nn}x_{n2}&=0\,.
  \end{split}
\end{equation*}
Vemos daqui que todos os $n$ sistemas lineares têm a mesma matriz de coeficientes, $\mb a$. Logo, para analisar todos eles de forma simultânea, podemos considerar matriz aumentada
$$[\mb a|\mb I_n]=\begin{bmatrix}
  a_{11}&\ldots&a_{1n}&|&1&\ldots&0\\
  \vdots&&\vdots&|&\vdots&&\vdots\\
  a_{n1}&\ldots&a_{nn}&|&0&\ldots&1
\end{bmatrix}\,.$$
Para achar os elementos da matriz $\mb x$ devemos usar o método de eliminação na matriz $[\mb a|\mb I_n]$ até transformá-la numa matriz $[\mb I_n|\mb b]$. Se isso for possível (se $\mb a$ for invertível), vamos ter $\mb x=\mb b$ e assim $\mb b=\mb a^{-1}$.

Exemplos:
\begin{enumerate}
  \item Seja a matriz
 $$\mb a=\begin{bmatrix}
   1&1&1\\
   2&-1&2\\
   1&-3&2
 \end{bmatrix}\,.$$
 Vamos provar que ela é uma matriz invertível e vamos encontrar a sua inversa. Para isso, consideramos
 \begin{multline*}
   \begin{bmatrix}
     1&1&1&|&1&0&0\\
     2&-1&2&|&0&1&0\\
     1&-3&2&|&0&0&1
   \end{bmatrix}\xrightarrow[L_3-L_1]{L_2-2L_1}\begin{bmatrix}
     1&1&1&|&1&0&0\\
     0&-3&0&|&-2&1&0\\
     0&-4&1&|&-1&0&1
   \end{bmatrix}\\
   \xrightarrow{L_2/(-3)}\begin{bmatrix}
     1&1&1&|&1&0&0\\
     0&1&0&|&2/3&-1/3&0\\
     0&-4&1&|&-1&0&1
   \end{bmatrix}\xrightarrow[L_3+4L_2]{L_1-L_2}\begin{bmatrix}
     1&0&1&|&1/3&1/3&0\\
     0&1&0&|&2/3&-1/3&0\\
     0&0&1&|&5/3&-4/3&1
   \end{bmatrix}\\
   \xrightarrow{L_1-L3}\begin{bmatrix}
     1&0&0&|&-4/3&5/3&-1\\
     0&1&0&|&2/3&-1/3&0\\
     0&0&1&|&5/3&-4/3&1
   \end{bmatrix}\,.
 \end{multline*}
 Portanto, $\mb a$ é invertível e
 $$\mb a^{-1}=\begin{bmatrix}
   -4/3&5/3&-1\\
   2/3&-1/3&0\\
   5/3&-4/3&1
 \end{bmatrix}\,.$$
 \item Seja a matriz
 $\mb a=\begin{bmatrix}
   \alpha&\beta\\
   \gamma&\delta
 \end{bmatrix}$
 tal que $D=\alpha\delta-\gamma\beta\ne 0$. Segue daqui que $\alpha\ne 0$ ou $\gamma\ne 0$. Supondo que $\alpha\ne 0$, temos que
 \begin{multline*}
   \begin{bmatrix}
     \alpha&\beta&|&1&0\\
     \gamma&\delta&|&0&1
   \end{bmatrix}\xrightarrow{L_1/\alpha}\begin{bmatrix}
     1&\beta/\alpha&|&1/\alpha&0\\
     \gamma&\delta&|&0&1
   \end{bmatrix}\xrightarrow{L_2-\gamma L_1}\begin{bmatrix}
     1&\beta/\alpha&|&1/\alpha&0\\
     0&D/\alpha&|&-\gamma/\alpha&1
   \end{bmatrix}\\
   \xrightarrow{[\alpha/D]L_2}\begin{bmatrix}
     1&\beta/\alpha&|&1/\alpha&0\\
     0&1&|&-\gamma/D&\alpha/D
   \end{bmatrix}\xrightarrow{L_1-(\beta/\alpha)L_2}\begin{bmatrix}
     1&0&|&\delta/D&-\beta/D\\
     0&1&|&-\gamma/D&\alpha/D
   \end{bmatrix}\,.
 \end{multline*}
 Portanto, a matriz $\mb a$ é invertível se $D\ne 0$ e $\alpha\ne 0$ e
 $$\mb a^{-1}=\frac{1}{D}\begin{bmatrix}
   \delta&-\beta\\
   -\gamma&\alpha
 \end{bmatrix}\,.$$
 De forma similar pode-se mostrar que $\mb a$ é invertível se $D\ne 0$ e $\gamma\ne 0$ e que $\mb a^{-1}$ é dada pela mesma expressão.
\end{enumerate}

\chapter{Determinantes}

Uma \tb{permutação} da lista $(1,\ldots,n)$ é uma lista $\pi=(\pi_1,\ldots,\pi_n)$, em que $\pi_i\in\{1,\ldots,n\}$ e $\pi_{i}\ne \pi_j$ para quaisquer $i,j\in\{1,\ldots,n\}$. A permutação $\pi$ também pode ser vista como uma bijeção $\pi:\{1,\ldots,n\}\to\{1,\ldots,n\}$.

Exemplo: As seguintes listas são permutações da lista $(1,2,3,4,5,6)$: $(1,3,5,2,4,6)$, $(2,4,1,5,6,3)$.

Denotaremos por $\mc P_n$ o conjunto de todas as permutações da lista $(1,\ldots,n)$. Podemos verificar facilmente que o número de elementos de $\mc P_n$ é $n!=n(n-1)\cdots 1$.

Realizar uma \tb{transposição} em uma lista de números é permutar qualquer par de elementos da lista.

Exemplo: A lista $(1,2,4,3,5)$ é obtida realizando uma transposição na lista $(1,2,5,3,4)$ que consiste em permutar os elementos $5$ e $4$.

\begin{thm*}
  Se $n\ge 2$, toda permutação da lista $(1,\ldots,n)$ pode ser obtida realizando um número finito de transposições na lista.
\end{thm*}
\begin{proof}
  Usamos indução em $n$. Se $n=2$, as únicas permutações da lista $(1,2)$ são $(1,2)$ e $(2,1)$, as quais podem ser obtidas realizando duas e uma transposições respectivamente. Suponhamos que o teorema seja verdadeiro para a lista $(1,\ldots,n)$. Se $\pi$ é uma permutação de $(1,\ldots,n,n+1)$, existe $i_0\in\{1,\ldots,n+1\}$ tal que $\pi_{i_0}=n+1$.
  Excluindo $\pi_{i_0}$ da lista $\pi$, obtemos uma permutação $\sigma$ de $(1,\ldots,n)$, a qual pode ser obtida realizando um número finito de transposições. Como claramente a lista $\pi$ pode ser obtida usando um número finito de transposições na lista $(\sigma_1,\ldots,\sigma_n,n+1)$, o teorema é verdadeiro para $n+1$. Portanto, o teorema é verdadeiro para qualquer $n\ge 2$.
\end{proof}

\begin{thm*}
  Se uma permutação $\pi$ da lista $(1,\ldots,n)$ é obtida realizando um número par (ímpar) de transposições específicas, então qualquer sequência de transposições aplicadas na lista $(1,\ldots,n)$ que leve à permutação $\pi$ terá também um número par (ímpar) de transposições.
\end{thm*}
\begin{proof}
  Usamos indução em $n$. Se $n=2$, OK. Supondo que o teorema seja verdadeiro para algum $n\ge 2$, consideremos uma permutação $\pi$ da lista $(1,\ldots,n,n+1)$ que é obtida a partir dessa lista realizando um número par de transposições específicas. Se $\pi_{n+1}=n+1$, os primeiros $n$ elementos de $\pi$ formam uma permutação $\pi'$ da lista $(1,\ldots,n)$, a qual é claramente obtida usando as transposições específicas usadas na construção de $\pi$. Logo, pela nossa hipótese, qualquer sequência de transposições aplicadas na lista $(1,\ldots,n)$ que leve à permutação $\pi'$ terá também um número par de transposições. Isso implica que o mesmo acontece para a permutação $\pi$ devido a que $\pi_{n+1}=n+1$.
  Se, por outro lado, $\pi_{n+1}\ne n+1$, existe um índice $i_0$ tal que $\pi_{i_0}=n+1$. Logo, a permutação $\pi$ pode ser levada a uma permutação $\sigma=(\pi_1,\ldots,\pi_{i_0-1},\pi_{n+1},\pi_{i_0+1},\ldots,n+1)$ pelo uso de uma única transposição.
  Segue daqui que $\sigma$ pode ser obtida a partir da lista $(1,\ldots,n,n+1)$ usando um número ímpar de transposições específicas e, por conseguinte, qualquer sequência de transposições aplicadas na lista $(1,\ldots,n,n+1)$ que leve à permutação $\sigma$ terá também um número ímpar de transposições. Portanto, qualquer sequência de transposições aplicadas na lista $(1,\ldots,n,n+1)$ que leve à permutação $\pi$ terá um número par de transposições.
\end{proof}


Diz-se que uma permutação é \tb{par} (\tb{ímpar}) se ela pode ser obtida realizando um número par (ímpar) de transposições.

Exemplos: Dada a lista $(1,2,3,4)$, a lista $(2,3,4,1)$ é uma permutação ímpar enquanto que a lista $(3,1,2,4)$ é uma permutação par.

Define-se o \tb{sinal} de uma permutação $\pi$ por
$$\sgn(\pi)=\begin{cases}
  1&\text{se $\pi$ é uma permutação par}\\
  -1&\text{se $\pi$ é uma permutação ímpar.}
\end{cases}$$

Define-se o \tb{determinante} de uma matriz $\mb a$ de ordem $n\times n$ por
$$\det(\mb a)=\begin{vmatrix}
  a_{11}&\ldots& a_{1n}\\
  \vdots&&\vdots\\
  a_{n1}&\ldots&a_{nn}
\end{vmatrix}=\sum_{\pi\in\mc P_n}\sgn(\pi)a_{1\pi_1}a_{2\pi_2}\cdots a_{n\pi_n}\,.$$

Exemplos:
\begin{enumerate}
  \item Se $\mb a$ é uma matriz $2\times 2$, então
 $$\det(\mb a)=\sgn(1,2)a_{11}a_{22}+\sgn(2,1)a_{12}a_{21}=a_{11}a_{22}-a_{21}a_{12}\,.$$
 Em outros termos, se
 $$\mb a=\begin{bmatrix}
   \alpha&\beta\\
   \gamma&\delta
 \end{bmatrix}\,,$$
 então
 $$\det(\mb a)=\begin{vmatrix}
   \alpha&\beta\\
   \gamma&\delta
 \end{vmatrix}=\alpha\delta-\gamma\beta\,.$$
 \item Temos que
 $$\begin{vmatrix}
   3&-4\\
   1&-2
 \end{vmatrix}=3\cdot (-2)-1\cdot (-4)=-2\,.$$
 \item Se $\mb a$ é uma matriz $n\times n$ que tem uma linha nula, então $\det(\mb a)=0$.
\end{enumerate}

\begin{lem*}
  \label{perm.nonid}
  Se $\pi$ é uma permutação de $(1,\ldots,n)$ tal que $\pi_i\ne i$ para algum índice $i$, então existem índices $j$ e $k$ tais que $\pi_j<j$ e $\pi_k>k$.
\end{lem*}
\begin{proof}
  Usamos indução em $n$. Se $n=2$, OK. Supondo que o lema seja verdadeiro para algum $n\ge 2$, consideremos uma permutação $\pi$ da lista $(1,\ldots,n,n+1)$ tal que $\pi_i\ne i$ para algum índice $i$. Se $\pi_{n+1}=n+1$, os primeiros $n$ números de $\pi$ conformam uma permutação $\pi'$ de $(1,\ldots,n)$ tal que $\pi'_i\ne i$ para algum índice $i$.
  Logo, pela nossa hipótese, existem índices $j$ e $k$ tais que $\pi_j<j$ e $\pi_k>k$.
  Por outro lado, se $\pi_{n+1}<n+1$, existe um índice $i_0<n+1$ tal que $\pi_{i_0}=n+1$, ou seja, $\pi_{i_0}>i_0$. Portanto, o lema é verdadeiro para qualquer $n\ge 2$.
\end{proof}

\begin{thm}
  Se $\mb a$ é uma matriz triangular $n\times n$, então $\det(\mb a)=a_{11}a_{22}\ldots a_{nn}$, ou seja, o determinante de uma matriz triangular é igual ao produto dos elementos da sua diagonal.
\end{thm}
\begin{proof}
  Se $\mb a$ é uma matriz triangular superior $n\times n$, então $a_{ij}=0$ se $i>j$. Logo, usando o lema~\ref{perm.nonid}, podemos concluir que
  $$\det(\mb a)=\sum_{\pi\in\mc P_n}\sgn(\pi)a_{1\pi_1}a_{2\pi_2}\cdots a_{n\pi_n}=a_{11}a_{22}\ldots a_{nn}\,,$$
  pois toda permutação $\pi\in\mc P_n$, $\pi\ne (1,\ldots,n)$, contribui com um termo nulo no somatório. De maneira análoga podemos provar que, se $\mb a$ é uma matriz triangular inferior $n\times n$, seu determinante é dado por $\det(\mb a)=a_{11}a_{22}\cdots a_{nn}$.
\end{proof}

\begin{thm}
  Seja $\mb a$ uma matriz $n\times n$. Tem-se que $\det (\mb a)=\det (\mb a^T)$.
\end{thm}
\begin{proof}
  Temos $\det(\mb a)=\sum_{\pi\in\mc P_n}\sgn(\pi)a_{1\pi_1}a_{2\pi_2}\cdots a_{n\pi_n}$. Se $\pi\in\mc P_n$ é uma permutação par, $\pi$ pode ser obtida realizando um número par de transposições na lista $(1,\ldots,n)$. Logo, a lista $\pi$ pode ser transformada na lista $(1,\ldots,n)$ realizando as transposições inversas, que também são em número par.
  Assim, podemos construir uma permutação $\sigma\in\mc P_n$ tal que $\sgn(\pi)a_{1\pi_1}a_{2\pi_2}\cdots a_{n\pi_n}=\sgn(\sigma)a_{\sigma_11}a_{\sigma_22}\cdots a_{\sigma_nn}$ e $\sgn(\sigma)=\sgn(\pi)$. Se $\pi\in\mc P_n$ é uma permutação ímpar, podemos proceder de forma análoga. Dessa maneira,
  $$\det(\mb a)=\sum_{\sigma\in\mc P_n}\sgn(\sigma)a_{\sigma_11}a_{\sigma_22}\cdots a_{\sigma_nn}=\sum_{\sigma\in\mc P_n}\sgn(\sigma)a^T_{1\sigma_1}a^T_{2\sigma_2}\cdots a^T_{n\sigma_n}\,.$$
  Portanto, $\det(\mb a)=\det(\mb a^T)$.
\end{proof}

\begin{thm}
  \label{det.anti}
  Seja $\mb a$ uma matriz $n\times n$. Se $\mb b$ é a matriz obtida ao permutar duas linhas (colunas) da matriz $\mb a$, então $\det(\mb b)=-\det(\mb a)$.
\end{thm}
\begin{proof}
  Suponhamos que $\mb b$ seja obtida ao permutar as linhas $i$ e $j$, $i<j$, da matriz $\mb a$. Dado $\pi\in\mc P_n$, temos que
  \begin{equation*}
    \begin{split}
      \sgn(\pi)b_{1\pi_1}\cdots b_{i\pi_i}\cdots b_{j\pi_j}\cdots b_{n\pi_n}&=\sgn(\pi)a_{1\pi_1}\cdots a_{j\pi_i}\cdots a_{i\pi_j}\cdots a_{n\pi_n}\\
      &=\sgn(\pi)a_{1\pi_1}\cdots a_{i\pi_j}\cdots a_{j\pi_i}\cdots a_{n\pi_n}
    \end{split}
  \end{equation*}
  Definindo a permutação $\sigma=(\pi_1,\ldots,\pi_j,\ldots,\pi_i,\ldots,\pi_n)$, vemos que $\sgn(\sigma)=-\sgn(\pi)$. Logo,
  $$\sgn(\pi)b_{1\pi_1}\cdots b_{i\pi_i}\cdots b_{j\pi_j}\cdots b_{n\pi_n}=-\sgn(\sigma)a_{1\sigma_1}\cdots a_{i\sigma_i}\cdots a_{j\sigma_j}\cdots a_{n\sigma_n}\,.$$
  Dessa maneira,
  $$\det(\mb b)=\sum_{\pi\in\mc P_n}\sgn(\pi)b_{1\pi_1}b_{2\pi_2}\cdots b_{n\pi_n}=-\sum_{\sigma\in\mc P_n}\sgn(\sigma)a_{1\sigma_1}a_{2\sigma_2}\cdots a_{n\sigma_n}\,,$$
  ou seja, $\det(\mb b)=-\det(\mb a)$.
\end{proof}
\begin{cor}
  Se uma matriz quadrada $\mb a$ tem duas linhas (colunas) iguais, então $\det(\mb a)=0$.
\end{cor}

\begin{thm}
  \label{det.lin}
  Para qualquer número $\alpha$ tem-se que
  $$\begin{vmatrix}
    a_{11}&\ldots& a_{1n}\\
    \vdots&&\vdots\\
    \alpha a_{i1}&\ldots&\alpha a_{in}\\
    \vdots&&\vdots\\
    a_{n1}&\ldots&a_{nn}
  \end{vmatrix}=\alpha\begin{vmatrix}
    a_{11}&\ldots& a_{1n}\\
    \vdots&&\vdots\\
    a_{i1}&\ldots& a_{in}\\
    \vdots&&\vdots\\
    a_{n1}&\ldots&a_{nn}
  \end{vmatrix}\,.$$
  Por outro lado,
  $$\begin{vmatrix}
    a_{11}&\ldots& a_{1n}\\
    \vdots&&\vdots\\
    a_{i1}+b_1&\ldots&a_{in}+b_n\\
    \vdots&&\vdots\\
    a_{n1}&\ldots&a_{nn}
  \end{vmatrix}=\begin{vmatrix}
    a_{11}&\ldots& a_{1n}\\
    \vdots&&\vdots\\
    a_{i1}&\ldots& a_{in}\\
    \vdots&&\vdots\\
    a_{n1}&\ldots&a_{nn}
  \end{vmatrix}+\begin{vmatrix}
    a_{11}&\ldots& a_{1n}\\
    \vdots&&\vdots\\
    b_1&\ldots& b_n\\
    \vdots&&\vdots\\
    a_{n1}&\ldots&a_{nn}
  \end{vmatrix}\,.$$
  Em outras palavras, o determinante de uma matriz $n\times n$ é uma função $n$-linear das linhas da matriz. Da mesma maneira, esse determinante é também uma função $n$-linear das colunas da matriz.
\end{thm}
\begin{proof}
  A primeira igualdade segue imediatamente de observarmos que
  $$\begin{vmatrix}
    a_{11}&\ldots& a_{1n}\\
    \vdots&&\vdots\\
    \alpha a_{i1}&\ldots&\alpha a_{in}\\
    \vdots&&\vdots\\
    a_{n1}&\ldots&a_{nn}
  \end{vmatrix}=\sum_{\pi\in\mc P_n}\sgn(\pi)a_{1\pi_1}\cdots (\alpha a_{i\pi_i})\cdots a_{n\pi_n}
  =\alpha\sum_{\pi\in\mc P_n}\sgn(\pi)a_{1\pi_1}\cdots a_{i\pi_i}\cdots a_{n\pi_n}\,.$$
  De forma análoga, a segunda igualdade do teorema segue diretamente de notarmos que
  \begin{equation*}
    \begin{split}
      \begin{vmatrix}
        a_{11}&\ldots& a_{1n}\\
        \vdots&&\vdots\\
        a_{i1}+b_1&\ldots&a_{in}+b_n\\
        \vdots&&\vdots\\
        a_{n1}&\ldots&a_{nn}
      \end{vmatrix}&=\sum_{\pi\in\mc P_n}\sgn(\pi)a_{1\pi_1}\cdots (a_{i\pi_i}+b_{\pi_i})\cdots a_{n\pi_n}\\
      &=\sum_{\pi\in\mc P_n}\sgn(\pi)a_{1\pi_1}\cdots a_{i\pi_i}\cdots a_{n\pi_n}+\sum_{\pi\in\mc P_n}\sgn(\pi)a_{1\pi_1}\cdots b_{\pi_i}\cdots a_{n\pi_n}\,.\qedhere
    \end{split}
  \end{equation*}
\end{proof}

\begin{cor}
  O determinante de uma matriz quadrada não muda se somamos a uma de suas linhas (colunas) um múltiplo de outra linha (coluna) da matriz.
\end{cor}

Exemplos:
\begin{enumerate}
  \item Temos que
  $$\begin{vmatrix}
    3&9&6\\
    5&3&1\\
    1&3&2
  \end{vmatrix}=3\begin{vmatrix}
    1&3&2\\
    5&3&1\\
    1&3&2
  \end{vmatrix}=0\,.$$
  \item Temos que
  $$\begin{vmatrix}
    3&1&6\\
    6&3&1\\
    0&0&2
  \end{vmatrix}=\begin{vmatrix}
    3&1&6\\
    6-2\cdot 3&3-2\cdot1&1-2\cdot 6\\
    0&0&2
  \end{vmatrix}=\begin{vmatrix}
    3&1&6\\
    0&1&-11\\
    0&0&2
  \end{vmatrix}=6\,.$$
  Por outro lado, temos também que
  $$\begin{vmatrix}
    3&1&6\\
    6&3&1\\
    0&0&2
  \end{vmatrix}=\begin{vmatrix}
    3-2\cdot 1&1&6\\
    6-2\cdot 3&3&1\\
    0-2\cdot 0&0&2
  \end{vmatrix}=\begin{vmatrix}
    1&1&6\\
    0&3&1\\
    0&0&2
  \end{vmatrix}=6\,.$$
\end{enumerate}

\begin{lem*}
  \label{det.minor1}
  Seja $\mb a$ uma matriz $n\times n$ tal que sua $p$-ésima linha ($q$-ésima coluna) tenha seus elementos nulos com a exceção do elemento $a_{pq}=1$. Se $\mb M_{pq}$ é a matriz obtida removendo a linha $p$ e a coluna $q$ da matriz $\mb a$, então $\det(\mb a)=(-1)^{p+q}\det(\mb M_{pq})$.
\end{lem*}
\begin{proof}
  Suponhamos que $p=1$. Logo,
  $$a_{1j}=\begin{cases}
    1&\text{se $j=q$}\\
    0&\text{se $j\ne q$.}
  \end{cases}$$
  Usando isso temos que
  $$\det(\mb a)=\sum_{\pi\in\mc P_n}\sgn(\pi)a_{1\pi_1}a_{2\pi_2}\cdots a_{n\pi_n}=\sum_{\substack{\pi\in\mc P_n\\\pi_1=q}}\sgn(\pi)a_{1q}a_{2\pi_2}\cdots a_{n\pi_n}\,,$$
  em que o último somatório é sobre todas as permutações $\pi\in\mc P_n$ que tem primeira componente igual a $q$. Cada uma dessas permutações tem a forma $(q,\sigma)$, em que $\sigma$ é uma permutação da lista $(1,\ldots,q-1,q+1,\ldots,n)$.
  A permutação $\sigma$ pode ser transformada na lista $(1,\ldots,q-1,q+1,\ldots,n)$ realizando um número par ou ímpar de transposições $l$. Logo, em qualquer caso, para transformar a permutação $(q,\sigma)\in\mc P_n$ na lista $(1,\ldots,q-1,q,q+1,\ldots,n)$, devemos realizar $l+q-1$ transposições. Segue daqui que $\sgn(q,\sigma)=(-1)^{l+q-1}=(-1)^{q-1}\sgn(\sigma)$.
  Como a lista $(1,\ldots,q-1,q+1,\ldots,n)$ tem $n-1$ elementos, cada permutação $\sigma$ dessa lista pode ser associada de forma biunívoca com uma permutação $\tau\in\mc P_{n-1}$ que realiza as mesmas transposições na lista $(1,\ldots,n-1)$ e assim $\sgn(\sigma)=\sgn(\tau)$. Dessa maneira, se $\mb M_{1q}$ é a matriz obtida removendo a primeira linha e a coluna $q$ da matriz $\mb a$, temos que
  $$\det(\mb a)=\sum_{\tau\in\mc P_{n-1}}(-1)^{q-1}\sgn(\tau)(M_{1q})_{1\tau_1}\cdots (M_{1q})_{n-1,\tau_{n-1}}=(-1)^{q-1}\det(\mb M_{1q})\,.$$
  Finalmente, se $p\ne 1$, realizando $p-1$ permutações de pares de linhas de $\mb a$ podemos cair no caso anterior e assim $\det(\mb a)=(-1)^{p-1}(-1)^{q-1}\det(\mb M_{pq})=(-1)^{p+q}\det(\mb M_{pq})$.
\end{proof}

\begin{thm}
  Seja $\mb a$ uma matriz $n\times n$. Se $\mb M_{pq}$ é a matriz obtida removendo a linha $p$ e a coluna $q$ da matriz $\mb a$, então para quaisquer $i,k\in\{1,\ldots,n\}$ tem-se
  $$\det(\mb a)=\sum_{j=1}^n(-1)^{i+j}a_{ij}\det(\mb M_{ij})=\sum_{j=1}^n(-1)^{j+k}a_{jk}\det(\mb M_{jk})\,.$$
\end{thm}
\begin{proof}
  Em virtude do teorema~\ref{det.lin} temos que
  \begin{multline*}
    \begin{vmatrix}
      a_{11}&a_{12}&\ldots&a_{1n}\\
      \vdots&\vdots&&\vdots\\
      a_{i1}&a_{i2}&\ldots&a_{in}\\
      \vdots&\vdots&&\vdots\\
      a_{n1}&a_{n2}&\ldots&a_{nn}
    \end{vmatrix}=a_{i1}\begin{vmatrix}
      a_{11}&a_{12}&\ldots&a_{1n}\\
      \vdots&\vdots&&\vdots\\
      1&0&\ldots&0\\
      \vdots&\vdots&&\vdots\\
      a_{n1}&a_{n2}&\ldots&a_{nn}
    \end{vmatrix}+a_{i2}\begin{vmatrix}
      a_{11}&a_{12}&\ldots&a_{1n}\\
      \vdots&\vdots&&\vdots\\
      0&1&\ldots&0\\
      \vdots&\vdots&&\vdots\\
      a_{n1}&a_{n2}&\ldots&a_{nn}
    \end{vmatrix}+\cdots\\
    +a_{in}\begin{vmatrix}
      a_{11}&a_{12}&\ldots&a_{1n}\\
      \vdots&\vdots&&\vdots\\
      0&0&\ldots&1\\
      \vdots&\vdots&&\vdots\\
      a_{n1}&a_{n2}&\ldots&a_{nn}
    \end{vmatrix}\,.
  \end{multline*}
  Logo, pelo lema~\ref{det.minor1}, temos que
  $$\det(\mb a)=a_{i1}(-1)^{i+1}\det(\mb M_{i1})+a_{i2}(-1)^{i+2}\det(\mb M_{i2})+\cdots+a_{in}(-1)^{i+n}\det(\mb M_{in})$$
  e assim $\det(\mb a)=\sum_{j=1}^n(-1)^{i+j}a_{ij}\det(\mb M_{ij})$.
\end{proof}

Exemplo: Temos que
  \begin{equation*}
    \begin{split}
      \begin{vmatrix}
        a_{11}&a_{12}&a_{13}\\
        a_{21}&a_{22}&a_{23}\\
        a_{31}&a_{32}&a_{33}
      \end{vmatrix}&=a_{11}\begin{vmatrix}
        a_{22}&a_{23}\\
        a_{32}&a_{33}
      \end{vmatrix}-a_{12}\begin{vmatrix}
        a_{21}&a_{23}\\
        a_{31}&a_{33}
      \end{vmatrix}+a_{13}\begin{vmatrix}
        a_{21}&a_{22}\\
        a_{31}&a_{32}
      \end{vmatrix}\\
      &=a_{11}(a_{22}a_{33}-a_{32}a_{23})-a_{12}(a_{21}a_{33}-a_{31}a_{23})+a_{13}(a_{21}a_{32}-a_{31}a_{22})\\
      &=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-(a_{13}a_{22}a_{31}+a_{11}a_{23}a_{32}+a_{12}a_{21}a_{33})\,.
    \end{split}
  \end{equation*}

% \begin{cor}
%   Os teoremas~\ref{det.anti} e~\ref{det.lin} assim como seus corolários continuam sendo verdadeiros se trocamos a palavra linha por coluna.
% \end{cor}
%
% \begin{cor}
%   Seja $\mb a$ uma matriz $n\times n$. Se $\mb M_{pq}$ é a matriz obtida removendo a linha $p$ e a coluna $q$ da matriz $\mb a$, então para qualquer $j\in\{1,\ldots,n\}$ tem-se
%   $$\det(\mb a)=\sum_{i=1}^n(-1)^{i+j}a_{ij}\det(\mb M_{ij})\,.$$
% \end{cor}

Exemplo: Temos que
$$\begin{vmatrix}
  1&2&3&4\\
  0&4&0&0\\
  0&5&1&1\\
  1&3&2&0
\end{vmatrix}=4 \begin{vmatrix}
  1&3&4\\
  0&5&1\\
  1&2&0
\end{vmatrix}=4\begin{vmatrix}
  0&1&4\\
  0&5&1\\
  1&2&0
\end{vmatrix}=4\cdot 1 \begin{vmatrix}
  1&4\\
  5&1
\end{vmatrix}=4(1-20)=-76\,.$$

\begin{thm}
  Uma matriz $\mb a$ de ordem $n\times n$ é invertível se, e somente se, $\det(\mb a)\ne 0$.
\end{thm}
\begin{proof}
  ($\Rightarrow$) Se $\mb a$ é invertível, então $\posto(\mb a)=n$. Usando o método de eliminação podemos transformar a matriz $\mb a$ em uma matriz escalonada $\mb b$.
  A matriz $\mb b$ satisfaz as condições $\posto(\mb b)=n$ e $\det(\mb b)=\alpha\det(\mb a)$, em que $\alpha\ne 0$ é um número. Segue da primeira condição que os elementos da diagonal de $\mb b$ são não-nulos. Logo, como $\mb b$ é de fato uma matriz triangular superior, temos que $\det(\mb b)\ne 0$. Portanto, $\det(\mb a)\ne 0$.
  ($\Leftarrow$) Se $\mb a$ não é invertível, então $\posto(\mb a)<n$. Logo, se $\mb b$ é a matriz escalonada obtida usando o método de eliminação na matriz $\mb a$, $\mb b$ deve ter pelo menos uma linha nula. Isso implica que $\det(\mb b)=0$. Como $\det(\mb b)=\alpha\det(\mb a)$ para algum número $\alpha\ne 0$, segue que $\det(\mb a)=0$.
\end{proof}



\end{document}
