\documentclass[12pt,a4paper]{report}
\usepackage[portuguese]{babel}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[colorlinks,allcolors=blue]{hyperref}
\usepackage{microtype}
\usepackage{graphicx}

\let\emptyset=\varnothing

\newcommand{\tb}{\textbf}
\newcommand{\tbu}[1]{\tb{\textup{#1}}}
\newcommand{\mb}{\mathbf}
\newcommand{\mc}{\mathcal}

\newcommand{\dpar}[1]{\left(#1\right)}
\newcommand{\dsqr}[1]{\left[#1\right]}
\newcommand{\dcur}[1]{\left\{#1\right\}}
\newcommand{\dabs}[1]{\left|#1\right|}
\newcommand{\ang}[1]{\left\langle#1\right\rangle}

\newcommand{\ds}{\displaystyle}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}


\newtheorem{thm}{Teorema}[chapter]
\newtheorem{lem}[thm]{Lema}
\newtheorem{cor}[thm]{Corolário}
\newtheorem{thm*}[thm]{Teorema*}
\newtheorem{lem*}[thm]{Lema*}
\newtheorem{cor*}[thm]{Corolário*}

\DeclareMathOperator{\sen}{sen}
\DeclareMathOperator{\arcsen}{arc sen}
\DeclareMathOperator{\senh}{senh}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\posto}{posto}
\DeclareMathOperator{\sgn}{sgn}

\title{Notas de álgebra linear}
\author{Max Jáuregui}

%\linespread{1.2}
%\linespread{1.213} %11pt
%\linespread{1.241} %12pt

\begin{document}
\maketitle
  Estas notas foram criadas principalmente para meu uso pessoal e pode eventualmente ser usado como um curso elementar de álgebra linear. Todo o conteúdo foi produzido por mim, seguindo como roteiro o livro: E. L. Lima, \textit{Álgebra linear}, 8 ed. (IMPA, Rio de Janeiro, 2012).

  \begin{flushright}
    Max Jáuregui
  \end{flushright}
\tableofcontents

\chapter{Matrizes}

Uma \tb{matriz} de \tb{ordem} $m\times n$ é um arranjo retangular de $mn$ números reais ou complexos que tem $m$ linhas e $n$ colunas. Se $\mb{a}$ é uma matriz $m\times n$, então
$$\mb{a}=\begin{bmatrix}
  a_{11}&\ldots&a_{1n}\\
  \vdots&\vdots&\vdots\\
  a_{m1}&\ldots&a_{mn}\\
\end{bmatrix}\,,$$
em que, para cada $i\in\{1,\ldots,m\}$ e $j\in\{1,\ldots,n\}$, o número $a_{ij}$ é chamado de \tb{elemento} na posição $(i,j)$ da matriz $\mb{a}$.

Formatos especiais de matrizes:

\begin{enumerate}
  \item \tb{Matriz quadrada:} É uma matriz cujo número de linhas é igual ao seu número de colunas. Se $\mb{a}$ é uma matriz quadrada, os elementos $a_{ii}$ são chamados de elementos da \tb{diagonal} de $\mb{a}$. Exemplos:
  $$\begin{bmatrix}
    4&0\\
    2/3&-1
  \end{bmatrix}\,,\quad \begin{bmatrix}
    5&-2&1\\
    0&0&10\\
    2&1&\pi
  \end{bmatrix}\,.$$
  Os elementos da diagonal dessas matrizes são $(4,-1)$ e $(5,0,\pi)$ respectivamente.
  \item \tb{Matriz diagonal:} É uma matriz quadrada cujos elementos fora da diagonal são nulos. Exemplos:
  $$\begin{bmatrix}
    4&0\\
    0&-1
  \end{bmatrix}\,,\quad \begin{bmatrix}
    5&0&0\\
    0&0&0\\
    0&0&\pi
  \end{bmatrix}\,.$$
  \item \tb{Matriz identidade:} É uma matriz diagonal na qual todos os elementos da diagonal são iguais a $1$. A matriz identidade $n\times n$ é denotada por $\mb{I}_n$. Exemplos:
  $$\mb{I}_2=\begin{bmatrix}
    1&0\\
    0&1
  \end{bmatrix}\,,\quad \mb{I}_3=\begin{bmatrix}
    1&0&0\\
    0&1&0\\
    0&0&1
  \end{bmatrix}\,.$$
  \item \tb{Matriz triangular superior:} É uma matriz quadrada na qual todos os elementos debaixo da diagonal são nulos. Exemplos:
  $$\begin{bmatrix}
    4&3\\
    0&-1
  \end{bmatrix}\,,\quad \begin{bmatrix}
    5&3&0\\
    0&0&2\\
    0&0&\pi
  \end{bmatrix}\,.$$
  \item \tb{Matriz triangular inferior:} É uma matriz quadrada na qual todos os elementos acima da diagonal são nulos. Exemplos:
  $$\begin{bmatrix}
    4&0\\
    3&-1
  \end{bmatrix}\,,\quad \begin{bmatrix}
    5&0&0\\
    3&0&0\\
    0&2&\pi
  \end{bmatrix}\,.$$
  \item \tb{Matriz simétrica:} É uma matriz quadrada $\mb{a}$ tal que $a_{ij}=a_{ji}$ para quaisquer $i$ e $j$, ou seja, é uma matriz simétrica em relação à sua diagonal. Exemplos:
  $$\begin{bmatrix}
    4&3\\
    3&-1
  \end{bmatrix}\,,\quad \begin{bmatrix}
    5&3&0\\
    3&0&2\\
    0&2&\pi
  \end{bmatrix}\,.$$
  \item \tb{Matriz antissimétrica:} É uma matriz quadrada $\mb{a}$ tal que $a_{ij}=-a_{ji}$ para quaisquer $i$ e $j$. Segue daqui que $a_{ii}=-a_{ii}$ para todo $i$, o que implica que os elementos da diagonal de $\mb a$ são nulos. Exemplos:
  $$\begin{bmatrix}
    0&-3\\
    3&0
  \end{bmatrix}\,,\quad \begin{bmatrix}
    0&3&0\\
    -3&0&2\\
    0&-2&0
  \end{bmatrix}\,.$$
  \item \tb{Matriz hermitiana:} É uma matriz quadrada $\mb a$ tal que $a_{ij}=\overline{a_{ji}}$ para quaisquer $i$ e $j$, em que $\overline{a_{ji}}$ denota o complexo conjugado de $a_{ji}$ ($\overline{x+iy}=x-iy$, em que $i^2=-1$). Segue daqui que $a_{ii}=\overline{a_{ii}}$ para todo $i$, o que implica que os elementos da diagonal de $\mb a$ são reais. Exemplos:
  $$\begin{bmatrix}
    4&3i\\
    -3i&-1
  \end{bmatrix}\,,\quad \begin{bmatrix}
    5&3+i&0\\
    3-i&0&-2i\\
    0&2i&\pi
  \end{bmatrix}\,.$$
\end{enumerate}

Como veremos mais na frente, o conjunto de todas as matrizes $m\times n$ é um espaço vetorial, pois nele podem ser definidas duas operações: adição de matrizes e multiplicação de uma matriz por um número (real ou complexo dependendo das circunstâncias). As seguintes equações definem essas operações:
$$\mb{a}+\mb{b}=\begin{bmatrix}
  a_{11}+b_{11}&\ldots&a_{1n}+b_{1n}\\
  \vdots&\vdots&\vdots\\
  a_{m1}+b_{m1}&\ldots&a_{mn}+b_{mn}
\end{bmatrix}\,,\quad
\alpha\mb{a}=\begin{bmatrix}
  \alpha a_{11}&\ldots&\alpha a_{1n}\\
  \vdots&\vdots&\vdots\\
  \alpha a_{m1}&\ldots&\alpha a_{mn}
\end{bmatrix}\,.$$
Pode-se verificar facilmente que essas operações têm as seguintes propriedades:
\begin{enumerate}
  \item Associatividade: $\mb{a}+(\mb{b}+\mb{c})=(\mb{a}+\mb b)+\mb c$ e $\alpha(\beta \mb a)=(\alpha\beta)\mb a$.
  \item Comutatividade: $\mb a+\mb b=\mb b+\mb a$.
  \item Elemento neutro: Existe uma matriz $\mb 0$ tal que $\mb a+\mb 0=\mb a$ para qualquer $\mb a$.
  \item Elemento inverso: Para cada $\mb a$ existe uma matriz $-\mb a$ tal que $\mb a+(-\mb a)=\mb 0$.
  \item Multiplicação por $1$: $1\mb a=\mb a$.
  \item Distributividade: $\alpha(\mb a+\mb b)=\alpha\mb a+\alpha\mb b$ e $(\alpha+\beta)\mb a=\alpha\mb a+\beta\mb a$.
\end{enumerate}

Define-se a \tb{transposta} de uma matriz $\mb a$ como a matriz $\mb a^T$ cujas linhas são as colunas de $\mb a$, ou seja, $a_{ij}^T=a_{ji}$ para quaisquer $i$ e $j$. Segue diretamente dessa definição que $(\mb a^T)^T=\mb a$, $(\mb a+\mb b)^T=\mb a^T+\mb b^T$ e $(\alpha\mb a)^T=\alpha\mb a^T$.

Exemplos:

\begin{enumerate}
  \item Uma matriz $\mb a$ é simétrica (antissimétrica) se, e somente se, $\mb a=\mb a^T$ ($\mb a=-\mb a^T$).
  \item Uma matriz $\mb a$ é hermitiana se, e somente se, $\mb a^T=\overline{\mb a}$, em que $\overline{\mb a}$ é a matriz obtida ao tomar o complexo conjugado de cada elemento de $\mb a$.
  \item Toda matriz quadrada $\mb a$ pode ser escrita como a soma de uma matriz simétrica e uma matriz antissimétrica. Com efeito, definindo $\mb s=(\mb a+\mb a^T)/2$ e $\mb t=(\mb a-\mb a^T)/2$, temos que $\mb s=\mb s^T$, $\mb t=-\mb t^T$ e $\mb a=\mb s+\mb t$.
\end{enumerate}

Além das duas operações anteriores, pode ser definida uma \tb{multiplicação de matrizes}. Se $\mb a$ é uma matriz $m\times n$ e $\mb b$ é uma matriz $n\times p$, então a multiplicação de $\mb a$ com $\mb b$ pode ser efetuada e o produto é uma matriz $\mb c$ de ordem $m\times p$ tal que
$$c_{ik}=\sum_{j=1}^na_{ij}b_{jk}=a_{i1}b_{1k}+\cdots+a_{in}b_{nk}\,.$$
Em outras palavras, o elemento $c_{ik}$ da matriz produto $\mb c$ é obtido multiplicando cada elemento da linha $i$ da matriz $\mb a$ com o elemento respectivo na coluna $k$ da matriz $\mb b$ e somando esses produtos.

Exemplos:
\begin{enumerate}
  \item Dadas as matrizes
  $$\mb a=\begin{bmatrix}
    1&0&-3\\
    2&1&0
  \end{bmatrix}\quad\text{e}\quad\mb b=\begin{bmatrix}
    1&5&0\\
    4&-1&3\\
    0&2&0
  \end{bmatrix}\,,$$
  temos que
  $$\mb {ab}=\begin{bmatrix}
    1&-1&0\\
    6&9&3
  \end{bmatrix}\,.$$
  No entanto, o produto $\mb{ba}$ não está definido, pois o número de linhas de $\mb a$ é diferente do número de colunas de $\mb b$.
  \item Se $\mb a$ é uma matriz $m\times n$, o produto de $\mb a$ com $\mb I_n$ é o próprio $\mb a$ (isso justifica o nome de matriz identidade). Com efeito, os elementos da matriz $\mb I_n$ são dados pelo \tb{símbolo de Kronecker}
  $$\delta_{ij}=\begin{cases}
    1&\text{se $i=j$}\\
    0&\text{se $i\ne j$.}
  \end{cases}$$
  Logo, se $\mb c=\mb {aI}_n$, então $c_{ik}=\sum_{j=1}^na_{ij}\delta_{jk}=a_{ik}\delta_{kk}=a_{ik}$. De forma análoga pode-se provar que $\mb I_m\mb a=\mb a$.
  \item Duas matrizes não nulas podem ter como produto a matriz nula. Com efeito,
  $$\begin{bmatrix}
    1&0&1\\
    0&-1&2
  \end{bmatrix}\begin{bmatrix}
    -1&3\\
    2&-6\\
    1&-3
  \end{bmatrix}=\begin{bmatrix}
    0&0\\
    0&0
  \end{bmatrix}\,.$$
  Por outro lado
  $$\begin{bmatrix}
    -1&3\\
    2&-6\\
    1&-3
  \end{bmatrix}\begin{bmatrix}
    1&0&1\\
    0&-1&2
  \end{bmatrix}=\begin{bmatrix}
    -1&-3&5\\
    2&6&-10\\
    1&3&-5
  \end{bmatrix}\,,$$
  o que mostra que a multiplicação de matrizes não é comutativa.
  \item Mesmo a multiplicação de duas matrizes quadradas não é comutativa, pois, por exemplo,
  $$\begin{bmatrix}
    2&3\\
    -1&0
  \end{bmatrix} \begin{bmatrix}
    0&1\\
    1&0
  \end{bmatrix}=\begin{bmatrix}
    3&2\\
    0&-1
  \end{bmatrix}\quad\text{e}\quad\begin{bmatrix}
    0&1\\
    1&0
  \end{bmatrix} \begin{bmatrix}
    2&3\\
    -1&0
  \end{bmatrix}= \begin{bmatrix}
    -1&0\\
    2&3
  \end{bmatrix}\,.$$
\end{enumerate}

\begin{thm}
  Sejam $\mb a$ e $\mb b$ matrizes $m\times n$, $\mb c$ e $\mb d$ matrizes $n\times p$ e $\alpha$ um número. Tem-se que $\mb a(\mb b+\mb c)=\mb {ab}+\mb{ac}$, $(\mb a+\mb b)\mb c=\mb {ac}+\mb{bc}$ e $\mb a(\alpha\mb b)=\alpha (\mb {ab})=(\alpha \mb a)\mb b$.
\end{thm}
\begin{proof}
  Se $\mb p=\mb a(\mb b+\mb c)$, então
  $$p_{ik}=\sum_{j=1}^na_{ij}(b_{jk}+c_{jk})=\sum_{j=1}^na_{ij}b_{jk}+\sum_{j=1}^na_{ij}c_{jk}\,,$$
  o que implica que $\mb p=\mb {ab}+\mb{ac}$. A igualdade $(\mb a+\mb b)\mb c=\mb {ac}+\mb{bc}$ pode ser provada de forma análoga.
\end{proof}

Exemplo: Dada a matriz
$$\mb a=\begin{bmatrix}
  1&x\\
  0&1
\end{bmatrix}\,,$$
temos que
$$\mb a^n=\begin{bmatrix}
  1&nx\\
  0&1
\end{bmatrix}$$
para todo $n\in\N$. Com efeito, definindo
$$\mb t=\begin{bmatrix}
  0&x\\
  0&0
\end{bmatrix}$$
temos que $\mb a=\mb I_2+\mb t$ e que $\mb t^2=\mb 0$. Logo,
$$\mb a^2=(\mb I_2+\mb t)^2=(\mb I_2+\mb t)(\mb I_2+\mb t)=\mb I_2^2+\mb I_2\mb t+\mb t\mb I_2+\mb t^2=\mb I_2+2\mb t\,.$$
Supondo que $\mb a^n=\mb I_2+n\mb t$ para algum $n\in\N$, temos que
$$\mb a^{n+1}=\mb a^n\mb a=(\mb I_2+n\mb t)(\mb I_2+\mb t)=\mb I_2^2+\mb I_2\mb t+n\mb t\mb I_2+n\mb t^2=\mb I_2+(n+1)\mb t\,.$$
Portanto, $\mb a^n=\mb I_2+n\mb t$ para todo $n\in\N$.

\begin{thm}
  Sejam $\mb a$ uma matriz $m\times n$, $\mb b$ uma matriz $n\times p$ e $\mb c$ uma matriz $p\times q$. Tem-se que $\mb a(\mb {bc})=(\mb{ab})\mb c$.
\end{thm}
\begin{proof}
  Sejam $\mb r=\mb{ab}$, $\mb s=\mb {bc}$ e $\mb t=\mb {as}$. Temos que
  $$t_{ik}=\sum_{j=1}^na_{ij}s_{jk}=\sum_{j=1}^na_{ij}\sum_{l=1}^pb_{jl}c_{lk}=\sum_{j=1}^n\sum_{l=1}^pa_{ij}b_{jl}c_{lk}=\sum_{l=1}^p\dpar{\sum_{j=1}^na_{ij}b_{jl}}c_{lk}=\sum_{l=1}^pr_{il}c_{lk}\,.$$
  Portanto, $\mb t=\mb {rc}$.
\end{proof}

\begin{thm}
  Sejam $\mb a$ uma matriz $m\times n$ e $\mb b$ uma matriz $n\times p$. Tem-se que $(\mb a\mb b)^T=\mb b^T\mb a^T$.
\end{thm}
\begin{proof}
  Se $\mb c=\mb {ab}$, então
  $$c^T_{ik}=c_{ki}=\sum_{j=1}^na_{kj}b_{ji}=\sum_{j=1}^na_{jk}^Tb_{ij}^T=\sum_{j=1}^nb_{ij}^Ta_{jk}^T\,.$$
  Portanto, $\mb c^T=\mb b^T\mb a^T$.
\end{proof}

Define-se o \tb{traço} de uma matriz quadrada $\mb a$ como a soma dos elementos da sua diagonal. O traço de $\mb a$ é denotado por $\tr(\mb a)$. Segue imediatamente dessa definição que $\tr(\mb a)=\tr(\mb a^T)$, pois $\mb a$ e $\mb a^T$ têm a mesma diagonal. Além disso, $\tr(\mb a+\mb b)=\tr(\mb a)+\tr(\mb b)$ e $\tr(\alpha\mb a)=\alpha\tr(\mb a)$.

\begin{thm}
  Sejam $\mb a$ uma matriz $m\times n$ e $\mb b$ uma matriz $n\times m$. Tem-se que $\tr(\mb {ab})=\tr(\mb{ba})$.
\end{thm}
\begin{proof}
  Se $\mb c=\mb a\mb b$, então $\tr(\mb {ab})=\sum_{i=1}^m c_{ii}$. Como $c_{ii}=\sum_{j=1}^na_{ij}b_{ji}$, segue que
  $$\tr(\mb{ab})=\sum_{i=1}^m\sum_{j=1}^na_{ij}b_{ji}=\sum_{j=1}^n\sum_{i=1}^ma_{ij}b_{ji}=\sum_{j=1}^n\sum_{i=1}^mb_{ji}a_{ij}\,.$$
  Portanto, $\tr(\mb{ab})=\tr(\mb{ba})$.
\end{proof}

\chapter{O método de eliminação de Gauss}

Diz-se que uma matriz é \tb{escalonada} quando o primeiro elemento não-nulo de cada linha está à esquerda dos primeiros elementos não-nulos das linhas subsequentes. Dessa maneira, debaixo do primeiro elemento não-nulo de cada linha só se tem zeros. Exemplos:
$$\begin{bmatrix}
  2&-1&0&4\\
  0&2&3&-1\\
  0&0&0&5
\end{bmatrix}\,,\quad \begin{bmatrix}
  0&1&2&3&4\\
  0&0&0&3&5\\
  0&0&0&0&1\\
  0&0&0&0&0
\end{bmatrix}\,.$$

Vamos agora descrever o \tb{método de eliminação de Gauss} que nos permite transformar uma matriz qualquer em uma matriz escalonada. O método de eliminação consiste em usar sistematicamente as seguintes operações nas linhas da matriz até obter uma matriz escalonada:
\begin{enumerate}
  \item permutar duas linhas;
  \item multiplicar uma linha por um número diferente de $0$;
  \item somar a uma linha um múltiplo de uma outra linha.
\end{enumerate}
Essas operações são às vezes chamadas de \tb{operações elementares}.

Exemplo:
\begin{multline*}
  \begin{bmatrix}
    0&0&2&0\\
    0&1&3&1\\
    0&3&0&2
  \end{bmatrix}\xrightarrow{L_1\leftrightarrow L_2} \begin{bmatrix}
    0&1&3&1\\
    0&0&2&0\\
    0&3&0&2
\end{bmatrix}\xrightarrow{L_3-3L_1} \begin{bmatrix}
  0&1&3&1\\
  0&0&2&0\\
  0&0&-9&-1
\end{bmatrix}\xrightarrow{L_2/2}\begin{bmatrix}
  0&1&3&1\\
  0&0&1&0\\
  0&0&-9&-1
\end{bmatrix}\\
\xrightarrow{L_3+9L_2}\begin{bmatrix}
  0&1&3&1\\
  0&0&1&0\\
  0&0&0&-1
\end{bmatrix}\,.
\end{multline*}

Uma operação elementar na linha de uma matriz $\mb a$ pode ser vista também como o resultado da multiplicação de uma matriz $\mb e$ com $\mb a$. Com efeito, se
$$\mb{e}_1=\begin{bmatrix}
  0&1&0\\
  1&0&0\\
  0&0&1
\end{bmatrix}\quad\text{e}\quad \mb a=\begin{bmatrix}
  0&0&2&0\\
  0&1&3&1\\
  0&3&0&2
\end{bmatrix}\,,$$
temos que
$$\mb{e}_1\mb a=\begin{bmatrix}
  0&1&3&1\\
  0&0&2&0\\
  0&3&0&2
\end{bmatrix}\,,$$
ou seja, multiplicar $\mb e_1$ pela esquerda de $\mb a$ faz permutar as linhas $1$ e $2$ de $\mb a$. Se
$$\mb e_2=\begin{bmatrix}
  1&0&0\\
  0&1&0\\
  -3&0&1
\end{bmatrix}\,,$$
então
$$\mb e_2\mb e_1\mb a=\begin{bmatrix}
  0&1&3&1\\
  0&0&2&0\\
  0&0&-9&-1
\end{bmatrix}\,.$$
Logo, multiplicar $\mb e_2$ pela esquerda de $\mb e_1\mb a$ é equivalente a fazer a operação elementar $L_3-3L_1$ na matriz $\mb e_1\mb a$. Finalmente, se
$$\mb e_3=\begin{bmatrix}
  1&0&0\\
  0&1&0\\
  0&9/2&1
\end{bmatrix}\,,$$
então
$$\mb e_3\mb e_2\mb e_1\mb a=\begin{bmatrix}
  0&1&3&1\\
  0&0&2&0\\
  0&0&0&-1
\end{bmatrix}\,.$$
Dessa maneira a matriz $\mb a$ pode ser transformada numa matriz escalonada se multiplicamos a ela sucessivamente as matrizes $\mb e_1,\mb e_2,\mb e_3$ pela esquerda. As matrizes $\mb e_1,\mb e_2,\mb e_3$ são às vezes chamadas de \tb{matrizes elementares}, pois correspondem a operações elementares nas linhas da matriz $\mb a$.


Define-se o \tb{posto} de uma matriz como o número de linhas não-nulas da matriz escalonada obtida usando o método de eliminação. Segue imediatamente daqui que o posto de uma matriz $m\times n$ é no máximo igual a $m$. O posto de uma matriz não depende dos detalhes do processo de eliminação (isso será justificado mais na frente). Por exemplo, a matriz considerada no exemplo anterior tem posto $3$.

Diz-se que uma matriz $\mb a$ de ordem $n\times n$ é \tb{invertível} se existe uma matriz $\mb a^{-1}$ de ordem $n\times n$, chamada de \tb{inversa} da matriz $\mb a$, tal que $\mb a\mb a^{-1}=\mb a^{-1}\mb a=\mb I_n$. Como será justificado mais na frente, uma matriz quadrada é invertível se, e somente se, seu posto é igual ao seu número de colunas (ou linhas).

Exemplos:
\begin{enumerate}
  \item A matriz
  $$\mb a=\begin{bmatrix}
    1&2&3\\
    4&5&6\\
    7&8&9
  \end{bmatrix}$$
  não é invertível. Com efeito, temos que
  $$\begin{bmatrix}
    1&2&3\\
    4&5&6\\
    7&8&9
  \end{bmatrix}\xrightarrow[L_3-7L_1]{L_2-4L_1} \begin{bmatrix}
    1&2&3\\
    0&-3&-6\\
    0&-6&-12
  \end{bmatrix}\xrightarrow{L3-2L_2} \begin{bmatrix}
    1&2&3\\
    0&-3&-6\\
    0&0&0
  \end{bmatrix}\,,$$
  do qual segue que $\posto(\mb a)=2\ne 3$.
  \item A matriz
  $$\mb a=\begin{bmatrix}
    1&2&3&1\\
    0&4&0&0\\
    3&2&0&2\\
    1&-1&0&1
  \end{bmatrix}$$
  é invertível. Com efeito, temos que
  \begin{multline*}
    \begin{bmatrix}
      1&2&3&1\\
      0&4&0&0\\
      3&2&0&2\\
      1&-1&0&1
    \end{bmatrix}\xrightarrow[L_4-L_1]{L_3-3L_1} \begin{bmatrix}
      1&2&3&1\\
      0&4&0&0\\
      0&-4&-9&-1\\
      0&-3&-3&0
    \end{bmatrix}\xrightarrow[L_4+(3/4)L_2]{L_3+L_2} \begin{bmatrix}
      1&2&3&1\\
      0&4&0&0\\
      0&0&-9&-1\\
      0&0&-3&0
    \end{bmatrix}\\
    \xrightarrow{L_4-L_3/3}\begin{bmatrix}
      1&2&3&1\\
      0&4&0&0\\
      0&0&-9&-1\\
      0&0&0&1/3
    \end{bmatrix}\,,
  \end{multline*}
  o que implica que $\posto(\mb a)=4$.
\end{enumerate}

Um sistema linear de $m$ equações a $n$ incógnitas é dado por
\begin{equation}
  \label{sislin}
  \begin{split}
    a_{11}x_1+\cdots+a_{1n}x_n&=b_1\\
    \vdots\qquad\qquad&\quad\;\;\vdots\\
    a_{m1}x_1+\cdots+a_{mn}x_n&=b_m\,.
  \end{split}
\end{equation}
Diz-se que uma lista $(\alpha_1,\ldots,\alpha_n)$ de $n$ números é uma \tb{solução} do sistema~(\ref{sislin}) se ao substituir $x_i$ por $\alpha_i$, $i\in\{1,\ldots,n\}$, em (\ref{sislin}) obtém-se identidades.

Exemplo: O sistema linear
\begin{equation*}
  \begin{split}
    2x-y=0\\
    x+2y=5
  \end{split}
\end{equation*}
tem como única solução o par ordenado $(1,2)$, pois $2\cdot 1-2=0$ e $1+2\cdot 2=5$.

Um sistema linear pode ter uma única solução, pode ter infinitas soluções ou pode não ter solução. Por exemplo, o sistema linear
\begin{equation*}
  \begin{split}
    2x-y=0\\
    4x-2y=0
  \end{split}
\end{equation*}
tem infinitas soluções, pois o par ordenado $(\alpha,2\alpha)$ é uma solução para qualquer $\alpha\in\R$. Por outro lado, o sistema linear
\begin{equation*}
  \begin{split}
    2x-y=0\\
    4x-2y=1
  \end{split}
\end{equation*}
não tem solução.

Definindo as matrizes
$$\mb a=\begin{bmatrix}
  a_{11}&\ldots&a_{1n}\\
  \vdots&&\vdots\\
  a_{m1}&\ldots&a_{mn}
\end{bmatrix}\,,\quad \mb x=\begin{bmatrix}
  x_1\\
  \vdots\\
  x_n
\end{bmatrix}\quad \text{e}\quad \mb b=\begin{bmatrix}
  b_1\\
  \vdots\\
  b_m
\end{bmatrix}\,,$$
o sistema~(\ref{sislin}) pode ser escrito de forma matricial como $\mb a\mb x=\mb b$. O método de eliminação pode ser usado para determinar se esse sistema tem uma única solução ou se tem infinitas soluções ou se não tem solução. Além disso, nos casos em que há solução, podemos ainda encontrar elas de forma explícita.

Toda a informação contida no sistema~(\ref{sislin}) está contida na chamada \tb{matriz aumentada} do sistema:
$$[\mb a|\mb b]=\begin{bmatrix}
  a_{11}&\ldots&a_{1n}&|&b_1\\
  \vdots&&\vdots&|&\vdots\\
  a_{m1}&\ldots&a_{mn}&|&b_m
\end{bmatrix}\,.$$
As operações elementares que podem ser usadas nessa matriz são equivalentes a
\begin{enumerate}
  \item permutar duas equações do sistema;
  \item multiplicar uma equação do sistema por um número;
  \item somar a uma equação do sistema um múltiplo de outra equação do sistema.
\end{enumerate}
Usando o método de eliminação na matriz aumentada da matriz podemos encontrar o seu posto e simultaneamente o posto da matriz $\mb a$. Com isso temos os seguintes casos:
\begin{enumerate}
  \item se $\posto([\mb a|\mb b])=\posto(\mb a)=n$, o sistema tem uma única solução;
  \item se $\posto([\mb a|\mb b])=\posto(\mb a)<n$, o sistema tem infinitas soluções;
  \item se $\posto([\mb a|\mb b])\ne\posto(\mb a)$, o sistema não tem solução.
\end{enumerate}

Exemplos:
\begin{enumerate}
  \item Seja o sistema linear
  \begin{equation*}
    \begin{split}
      x+y+z&=3\\
      2x-y+2z&=3\\
      x-3y+2z&=0\,.
    \end{split}
  \end{equation*}
  A matriz aumentada desse sistema é
  $$[\mb a|\mb b]=\begin{bmatrix}
    1&1&1&|&3\\
    2&-1&2&|&3\\
    1&-3&2&|&0
  \end{bmatrix}\,.$$
  Logo,
  $$\begin{bmatrix}
    1&1&1&|&3\\
    2&-1&2&|&3\\
    1&-3&2&|&0
  \end{bmatrix}\xrightarrow[L_3-L1]{L_2-2L_1} \begin{bmatrix}
    1&1&1&|&3\\
    0&-3&0&|&-3\\
    0&-4&1&|&-3
  \end{bmatrix}\xrightarrow{L_4-(4/3)L_3} \begin{bmatrix}
    1&1&1&|&3\\
    0&-3&0&|&-3\\
    0&0&1&|&1
  \end{bmatrix}\,.$$
  Segue daqui que $\posto([\mb a|\mb b])=\posto(\mb a)=3$ e, por conseguinte, o sistema tem uma única solução. Para determinar essa solução escrevemos a matriz escalonada obtida como o seguinte sistema linear:
  \begin{equation*}
    \begin{split}
      x+y+z&=3\\
      -3y&=-3\\
      z&=1\,.
    \end{split}
  \end{equation*}
  Resolvendo esse sistema de baixo para cima, obtemos que a solução do sistema é $(1,1,1)$.
  \item Seja o sistema linear
  \begin{equation*}
    \begin{split}
      x+y+z&=3\\
      2x-y+2z&=3\\
      -x+5y-z&=3\,.
    \end{split}
  \end{equation*}
  A matriz aumentada desse sistema é
  $$[\mb a|\mb b]=\begin{bmatrix}
    1&1&1&|&3\\
    2&-1&2&|&3\\
    -1&5&-1&|&3
  \end{bmatrix}\,.$$
  Logo,
  $$\begin{bmatrix}
    1&1&1&|&3\\
    2&-1&2&|&3\\
    -1&5&-1&|&3
  \end{bmatrix}\xrightarrow[L_3+L_1]{L_2-2L_1} \begin{bmatrix}
    1&1&1&|&3\\
    0&-3&0&|&-3\\
    0&6&0&|&6
  \end{bmatrix}\xrightarrow{L_3+2L_2} \begin{bmatrix}
    1&1&1&|&3\\
    0&-3&0&|&-3\\
    0&0&0&|&0
  \end{bmatrix}\,.$$
  Segue daqui que $\posto([\mb a|\mb b])=\posto(\mb a)=2<3$. Assim, o sistema tem infinitas soluções. Para ver a forma dessas soluções escrevemos a matriz escalonada obtida como o seguinte sistema linear:
  \begin{equation*}
    \begin{split}
      x+y+z&=3\\
      -3y&=-3\,.
    \end{split}
  \end{equation*}
  Da segunda equação obtemos que $y=1$. Substituindo isso na primeira, obtemos que $z=2-x$. Logo, a lista $(\alpha,1,2-\alpha)$ é uma solução do sistema para todo $\alpha\in\R$.
  \item Seja o sistema linear
  \begin{equation*}
    \begin{split}
      x+y+z&=3\\
      2x-y+2z&=3\\
      -x+5y-z&=2\,.
    \end{split}
  \end{equation*}
  A matriz aumentada desse sistema é
  $$[\mb a|\mb b]=\begin{bmatrix}
    1&1&1&|&3\\
    2&-1&2&|&3\\
    -1&5&-1&|&2
  \end{bmatrix}\,.$$
  Logo,
  $$\begin{bmatrix}
    1&1&1&|&3\\
    2&-1&2&|&3\\
    -1&5&-1&|&2
  \end{bmatrix}\xrightarrow[L_3+L_1]{L_2-2L_1} \begin{bmatrix}
    1&1&1&|&3\\
    0&-3&0&|&-3\\
    0&6&0&|&5
  \end{bmatrix}\xrightarrow{L_3+2L_2} \begin{bmatrix}
    1&1&1&|&3\\
    0&-3&0&|&-3\\
    0&0&0&|&-1
  \end{bmatrix}\,.$$
  Segue daqui que $\posto([\mb a|\mb b])=3\ne 2=\posto(\mb a)$ e, por conseguinte, o sistema não tem solução. Uma outra forma de chegar nessa conclusão é escrevendo a matriz escalonada obtida como um sistema linear. Nesse sistema a terceira equação diz é $0=-1$, o que é absurdo.
\end{enumerate}

Como vimos anteriormente, o método de eliminação pode ser usado para determinar se uma matriz quadrada $\mb a$ é invertível. Além disso, no caso afirmativo, o método de eliminação pode ser usado para o cálculo efetivo da inversa de $\mb a$.

Sejam as matrizes
$$\mb a=\begin{bmatrix}
  a_{11}&\ldots&a_{1n}\\
  \vdots&&\vdots\\
  a_{n1}&\ldots&a_{nn}
\end{bmatrix}\quad\text{e}\quad \mb x=\begin{bmatrix}
  x_{11}&\ldots&x_{1n}\\
  \vdots&&\vdots\\
  x_{n1}&\ldots&x_{nn}
\end{bmatrix}\,.$$
Para que $\mb x$ seja a matriz inversa de $\mb a$, deve-se ter em particular que $\mb a\mb x=\mb I_n$. Logo, se queremos encontrar os elementos da matriz $\mb x$, temos que resolver $n$ sistemas lineares com $n$ equações e $n$ incógnitas. O primeiro sistema corresponderia à obtenção da primeira coluna de $\mb I_n$, ou seja,
\begin{equation*}
  \begin{split}
    a_{11}x_{11}+\cdots+a_{1n}x_{n1}&=1\\
    a_{21}x_{11}+\cdots+a_{2n}x_{n1}&=0\\
    \vdots\qquad\qquad&\quad\,\,\,\vdots\\
    a_{n1}x_{11}+\cdots+a_{nn}x_{n1}&=0\,.
  \end{split}
\end{equation*}
O segundo sistema, que corresponde à obtenção da segunda coluna de $\mb I_n$, é
\begin{equation*}
  \begin{split}
    a_{11}x_{12}+\cdots+a_{1n}x_{n2}&=0\\
    a_{21}x_{12}+\cdots+a_{2n}x_{n2}&=1\\
    \vdots\qquad\qquad&\quad\,\,\,\vdots\\
    a_{n1}x_{12}+\cdots+a_{nn}x_{n2}&=0\,.
  \end{split}
\end{equation*}
Vemos daqui que todos os $n$ sistemas lineares tem a mesma matriz de coeficientes, $\mb a$. Logo, para analisar todos eles de forma simultânea, podemos considerar matriz aumentada
$$[\mb a|\mb I_n]=\begin{bmatrix}
  a_{11}&\ldots&a_{1n}&|&1&\ldots&0\\
  \vdots&&\vdots&|&\vdots&&\vdots\\
  a_{n1}&\ldots&a_{nn}&|&0&\ldots&1
\end{bmatrix}\,.$$
Para achar os elementos da matriz $\mb x$ devemos usar o método de eliminação na matriz $[\mb a|\mb I_n]$ até transformá-la numa matriz $[\mb I_n|\mb b]$. Se isso for possível (se $\mb a$ for invertível), vamos ter $\mb x=\mb b$ e assim $\mb b=\mb a^{-1}$.

Exemplos:
\begin{enumerate}
  \item Seja a matriz
 $$\mb a=\begin{bmatrix}
   1&1&1\\
   2&-1&2\\
   1&-3&2
 \end{bmatrix}\,.$$
 Vamos provar que ela é uma matriz invertível e vamos encontrar a sua inversa. Para isso, consideramos
 \begin{multline*}
   \begin{bmatrix}
     1&1&1&|&1&0&0\\
     2&-1&2&|&0&1&0\\
     1&-3&2&|&0&0&1
   \end{bmatrix}\xrightarrow[L_3-L_1]{L_2-2L_1}\begin{bmatrix}
     1&1&1&|&1&0&0\\
     0&-3&0&|&-2&1&0\\
     0&-4&1&|&-1&0&1
   \end{bmatrix}\\
   \xrightarrow{L_2/(-3)}\begin{bmatrix}
     1&1&1&|&1&0&0\\
     0&1&0&|&2/3&-1/3&0\\
     0&-4&1&|&-1&0&1
   \end{bmatrix}\xrightarrow[L_3+4L_2]{L_1-L_2}\begin{bmatrix}
     1&0&1&|&1/3&1/3&0\\
     0&1&0&|&2/3&-1/3&0\\
     0&0&1&|&5/3&-4/3&1
   \end{bmatrix}\\
   \xrightarrow{L_1-L3}\begin{bmatrix}
     1&0&0&|&-4/3&5/3&-1\\
     0&1&0&|&2/3&-1/3&0\\
     0&0&1&|&5/3&-4/3&1
   \end{bmatrix}\,.
 \end{multline*}
 Portanto, $\mb a$ é invertível e
 $$\mb a^{-1}=\begin{bmatrix}
   -4/3&5/3&-1\\
   2/3&-1/3&0\\
   5/3&-4/3&1
 \end{bmatrix}\,.$$
 \item Seja a matriz
 $\mb a=\begin{bmatrix}
   \alpha&\beta\\
   \gamma&\delta
 \end{bmatrix}$
 tal que $D=\alpha\delta-\gamma\beta\ne 0$. Segue daqui que $\alpha\ne 0$ ou $\gamma\ne 0$. Supondo que $\alpha\ne 0$, temos que
 \begin{multline*}
   \begin{bmatrix}
     \alpha&\beta&|&1&0\\
     \gamma&\delta&|&0&1
   \end{bmatrix}\xrightarrow{L_1/\alpha}\begin{bmatrix}
     1&\beta/\alpha&|&1/\alpha&0\\
     \gamma&\delta&|&0&1
   \end{bmatrix}\xrightarrow{L_2-\gamma L_1}\begin{bmatrix}
     1&\beta/\alpha&|&1/\alpha&0\\
     0&D/\alpha&|&-\gamma/\alpha&1
   \end{bmatrix}\\
   \xrightarrow{[\alpha/D]L_2}\begin{bmatrix}
     1&\beta/\alpha&|&1/\alpha&0\\
     0&1&|&-\gamma/D&\alpha/D
   \end{bmatrix}\xrightarrow{L_1-(\beta/\alpha)L_2}\begin{bmatrix}
     1&0&|&\delta/D&-\beta/D\\
     0&1&|&-\gamma/D&\alpha/D
   \end{bmatrix}\,.
 \end{multline*}
 Portanto, a matriz $\mb a$ é invertível se $D\ne 0$ e $\alpha\ne 0$ e
 $$\mb a^{-1}=\frac{1}{D}\begin{bmatrix}
   \delta&-\beta\\
   -\gamma&\alpha
 \end{bmatrix}\,.$$
 De forma similar pode-se mostrar que $\mb a$ é invertível se $D\ne 0$ e $\gamma\ne 0$ e que $\mb a^{-1}$ é dada pela mesma expressão.
\end{enumerate}

\chapter{Determinantes}

Uma \tb{permutação} da lista $(1,\ldots,n)$ é uma lista $\pi=(\pi_1,\ldots,\pi_n)$, em que $\pi_i\in\{1,\ldots,n\}$ e $\pi_{i}\ne \pi_j$ para quaisquer $i,j\in\{1,\ldots,n\}$. A permutação $\pi$ também pode ser vista como uma bijeção $\pi:\{1,\ldots,n\}\to\{1,\ldots,n\}$.

Exemplo: As seguintes listas são permutações da lista $(1,2,3,4,5,6)$: $(1,3,5,2,4,6)$, $(2,4,1,5,6,3)$.

Denotaremos por $\mc P_n$ o conjunto de todas as permutações da lista $(1,\ldots,n)$. Podemos verificar facilmente que o número de elementos de $\mc P_n$ é $n!=n(n-1)\cdots 1$.

Realizar uma \tb{transposição} em uma lista de números é permutar qualquer par de elementos da lista.

Exemplo: A lista $(1,2,4,3,5)$ é obtida realizando transposição na lista $(1,2,5,3,4)$ que consiste em permutar os elementos $5$ e $4$.

\begin{thm}
  Se $n\ge 2$, toda permutação da lista $(1,\ldots,n)$ pode ser obtida realizando um número finito de transposições na lista.
\end{thm}
\begin{proof}
  Usamos indução em $n$. Se $n=2$, as únicas permutações da lista $(1,2)$ são $(1,2)$ e $(2,1)$, as quais podem ser obtidas realizando duas e uma transposições respectivamente. Suponhamos que o teorema seja verdadeiro para a lista $(1,\ldots,n)$. Se $\pi$ é uma permutação de $(1,\ldots,n,n+1)$, existe $i_0\in\{1,\ldots,n+1\}$ tal que $\pi_{i_0}=n+1$.
  Excluindo $\pi_{i_0}$ da lista $\pi$, obtemos uma permutação $\sigma$ de $(1,\ldots,n)$, a qual pode ser obtida realizando um número finito de transposições. Como claramente a lista $\pi$ pode ser obtida usando um número finito de transposições na lista $(\sigma_1,\ldots,\sigma_n,n+1)$, o teorema é verdadeiro para $n+1$. Portanto, o teorema é verdadeiro para qualquer $n\ge 2$.
\end{proof}

Diz-se que uma permutação é \tb{par} (\tb{ímpar}) se ela pode ser obtida realizando um número par (ímpar) de transposições.

Exemplos: Dada a lista $(1,2,3,4)$, a lista $(2,3,4,1)$ é uma permutação ímpar enquanto que a lista $(3,1,2,4)$ é uma permutação par.

Define-se o sinal de uma permutação $\pi$ por
$$\sgn(\pi)=\begin{cases}
  1&\text{se $\pi$ é uma permutação par}\\
  -1&\text{se $\pi$ é uma permutação ímpar.}
\end{cases}$$

\end{document}
